{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN with Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a personal curiosity, I'd like to try to squeeze as much performance as possible from the initial DQN application.\n",
    "\n",
    "I intend to do so using ensembling: train multiple agents with the same video and different subjects and then use **majority-voting** at test time, to see if we can get any performance improvements, even with an extremely simple approach!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tianshou as ts \n",
    "from tianshou.utils import TensorboardLogger\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johndoe/Desktop/uni/affectivecompute/project/lib/python3.11/site-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "from utils_preprocess import compute_frame_features, compute_foa_features\n",
    "\n",
    "from env_base import BaseEnvironment\n",
    "from env_base_test import BaseTestEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and environment initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_filename = \"012\"\n",
    "mat_filename = vid_filename + \".mat\"\n",
    "\n",
    "n_subjects = 10 # pick the first ten subjects (out of 39)\n",
    "subjects = [n for n in range(n_subjects)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_bounding_boxes_per_frame, patch_centres_per_frame, speaker_info_per_frame = compute_frame_features(\n",
    "    vid_filename\n",
    ")\n",
    "\n",
    "foa_centres_per_frame_per_subject, patch_weights_per_frame = compute_foa_features(\n",
    "    mat_filename, patch_centres_per_frame\n",
    ")\n",
    "\n",
    "foa_centres_per_frame_subjects = [\n",
    "    [frame[target_subject] for frame in foa_centres_per_frame_per_subject] \n",
    "    for target_subject in subjects\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_envs = []\n",
    "for subject in subjects:\n",
    "    markov_env = BaseEnvironment(\n",
    "        1,\n",
    "        patch_bounding_boxes_per_frame,\n",
    "        patch_centres_per_frame,\n",
    "        speaker_info_per_frame,\n",
    "        foa_centres_per_frame_subjects[subject],\n",
    "        patch_weights_per_frame,\n",
    "        frame_width=320, # from data_utils.py\n",
    "        frame_height=180,\n",
    "    )\n",
    "    markov_envs.append(markov_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_envs = []\n",
    "all_test_envs = []\n",
    "for markov_env in markov_envs:\n",
    "    num_train_envs = 5\n",
    "    num_test_envs = 10\n",
    "\n",
    "    train_envs = ts.env.DummyVectorEnv([lambda: markov_env for _ in range(num_train_envs)])\n",
    "    all_train_envs.append(train_envs)\n",
    "\n",
    "    test_envs = ts.env.DummyVectorEnv([lambda: markov_env for _ in range(num_test_envs)])\n",
    "    all_test_envs.append(test_envs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's construct the network.\n",
    "\n",
    "The biggest headache comes from the observations: they're quite complex. So, we build multiple networks, each processing a part of an observation and combining their outputs in the end!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, observation_space, action_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_patches = observation_space['patch_centres'].shape[0]\n",
    "\n",
    "        # network for patch_centres\n",
    "        self.patch_centres_net = nn.Sequential(\n",
    "            nn.Linear(np.prod(observation_space['patch_centres'].shape), 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # network for patch_bounding_boxes\n",
    "        self.patch_bboxes_net = nn.Sequential(\n",
    "            nn.Linear(np.prod(observation_space['patch_bounding_boxes'].shape), 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # network for speaker_info\n",
    "        self.speaker_info_net = nn.Sequential(\n",
    "            nn.Linear(np.prod(observation_space['speaker_info'].shape), 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # combining the outputs of all networks\n",
    "        self.combined_net = nn.Sequential(\n",
    "            nn.Linear(64 + 64 + 32, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, np.prod(action_shape))\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, state=None, info={}):\n",
    "        patch_centres = torch.tensor(obs['patch_centres'], dtype=torch.float32)\n",
    "        patch_bboxes = torch.tensor(obs['patch_bounding_boxes'], dtype=torch.float32)\n",
    "        speaker_info = torch.tensor(obs['speaker_info'], dtype=torch.float32)\n",
    "\n",
    "        patch_centres = patch_centres.view(patch_centres.size(0), -1)\n",
    "        patch_bboxes = patch_bboxes.view(patch_bboxes.size(0), -1)\n",
    "        speaker_info = speaker_info.view(speaker_info.size(0), -1)\n",
    "\n",
    "        # pass through respective networks\n",
    "        patch_centres_out = self.patch_centres_net(patch_centres)\n",
    "        patch_bboxes_out = self.patch_bboxes_net(patch_bboxes)\n",
    "        speaker_info_out = self.speaker_info_net(speaker_info)\n",
    "\n",
    "        # combine outputs\n",
    "        combined = torch.cat([patch_centres_out, patch_bboxes_out, speaker_info_out], dim=1)\n",
    "\n",
    "        logits = self.combined_net(combined)\n",
    "\n",
    "        return logits, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = []\n",
    "optims = []\n",
    "for markov_env in markov_envs:\n",
    "    state_shape = markov_env.observation_space\n",
    "    action_shape = markov_env.action_space.n\n",
    "\n",
    "    net = Net(state_shape, action_shape)\n",
    "    nets.append(net)\n",
    "\n",
    "    optim = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "    optims.append(optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to set up the policy, which is readily done in Tianshou."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = []\n",
    "for net, optim in zip(nets, optims):\n",
    "    policy = ts.policy.DQNPolicy(\n",
    "        model=net, \n",
    "        optim=optim, \n",
    "        discount_factor=0.99,\n",
    "        estimation_step=1,\n",
    "        target_update_freq=50\n",
    "    )\n",
    "    policies.append(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_collectors = []\n",
    "test_collectors = []\n",
    "for policy, train_env, test_envs in zip(policies, all_train_envs, all_test_envs):\n",
    "    train_collector = ts.data.Collector(policy, train_envs, ts.data.VectorReplayBuffer(1000, num_train_envs))\n",
    "    train_collectors.append(train_collector)\n",
    "\n",
    "    test_collector = ts.data.Collector(policy, test_envs)\n",
    "    test_collectors.append(test_collector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "num_steps_per_epoch = 1000\n",
    "step_per_collect = 10\n",
    "episode_per_test = 5\n",
    "batch_size = 30 # one second of data (videos are at 30FPS)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%d%m%Y-%H%M%S\")\n",
    "log_path = os.path.join(\"logs\", \"dqn\", \"base\", f\"video_{vid_filename}\", f\"ensemble_{n_subjects}\", timestamp)\n",
    "writer = SummaryWriter(log_path)\n",
    "logger = TensorboardLogger(writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[+] Training agent on subject 0...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 1001it [00:01, 504.21it/s, env_step=1000, len=79, loss=0.189, n/ep=0, n/st=10, rew=15.57]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: 66.027011 ± 35.927479, best_reward: 66.027011 ± 36.117924 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 1001it [00:01, 507.31it/s, env_step=2000, len=79, loss=0.263, n/ep=0, n/st=10, rew=15.65]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: 66.027011 ± 35.078453, best_reward: 66.027011 ± 36.117924 in #0\n",
      "\n",
      "[+] Training agent on subject 1...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 1001it [00:01, 509.47it/s, env_step=1000, len=79, loss=0.179, n/ep=0, n/st=10, rew=7.31]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: 67.202328 ± 34.142491, best_reward: 80.680116 ± 46.689854 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 1001it [00:01, 511.11it/s, env_step=2000, len=79, loss=0.308, n/ep=0, n/st=10, rew=13.44]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: 91.112050 ± 46.631109, best_reward: 91.112050 ± 46.631109 in #2\n",
      "\n",
      "[+] Training agent on subject 2...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 1001it [00:02, 482.47it/s, env_step=1000, len=79, loss=2.125, n/ep=0, n/st=10, rew=109.81]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: 264.982980 ± 135.818823, best_reward: 264.982980 ± 135.818823 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 1001it [00:02, 470.37it/s, env_step=2000, len=79, loss=1.869, n/ep=0, n/st=10, rew=107.02]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: 264.982980 ± 133.215542, best_reward: 264.982980 ± 135.818823 in #1\n",
      "\n",
      "[+] Training agent on subject 3...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 1001it [00:02, 470.88it/s, env_step=1000, len=79, loss=1.735, n/ep=0, n/st=10, rew=110.59]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: 166.800943 ± 84.879772, best_reward: 166.800943 ± 84.879772 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 1001it [00:02, 443.33it/s, env_step=2000, len=79, loss=2.969, n/ep=0, n/st=10, rew=84.28]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: 166.800943 ± 91.898487, best_reward: 166.800943 ± 84.879772 in #1\n",
      "\n",
      "[+] Training agent on subject 4...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 1001it [00:02, 475.94it/s, env_step=1000, len=79, loss=0.302, n/ep=0, n/st=10, rew=13.83]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: 92.854483 ± 49.966889, best_reward: 92.854483 ± 46.689806 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 1001it [00:02, 479.65it/s, env_step=2000, len=79, loss=0.422, n/ep=0, n/st=10, rew=3.61]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: 92.854483 ± 48.302384, best_reward: 92.854483 ± 46.689806 in #0\n",
      "\n",
      "[+] Training agent on subject 5...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 1001it [00:02, 459.23it/s, env_step=1000, len=79, loss=0.185, n/ep=0, n/st=10, rew=7.78]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: 54.517526 ± 30.803902, best_reward: 54.517526 ± 30.803902 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 1001it [00:02, 472.45it/s, env_step=2000, len=79, loss=0.257, n/ep=0, n/st=10, rew=4.40]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: 53.448926 ± 28.471738, best_reward: 54.517526 ± 30.803902 in #1\n",
      "\n",
      "[+] Training agent on subject 6...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 1001it [00:02, 468.86it/s, env_step=1000, len=79, loss=0.194, n/ep=0, n/st=10, rew=7.51]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: 51.825853 ± 23.482397, best_reward: 51.825853 ± 23.482397 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 1001it [00:02, 464.77it/s, env_step=2000, len=79, loss=0.308, n/ep=0, n/st=10, rew=8.34]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: 59.927268 ± 33.369349, best_reward: 59.927268 ± 33.369349 in #2\n",
      "\n",
      "[+] Training agent on subject 7...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 1001it [00:02, 468.19it/s, env_step=1000, len=79, loss=1.482, n/ep=0, n/st=10, rew=111.06]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: 219.137024 ± 111.328033, best_reward: 219.137024 ± 115.344070 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 1001it [00:02, 474.81it/s, env_step=2000, len=79, loss=4.837, n/ep=0, n/st=10, rew=107.48]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: 219.137024 ± 107.479633, best_reward: 219.137024 ± 115.344070 in #0\n",
      "\n",
      "[+] Training agent on subject 8...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 1001it [00:02, 461.69it/s, env_step=1000, len=79, loss=2.215, n/ep=0, n/st=10, rew=110.48]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: 286.436860 ± 147.444238, best_reward: 286.436860 ± 147.444238 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 1001it [00:02, 465.69it/s, env_step=2000, len=79, loss=6.717, n/ep=0, n/st=10, rew=108.06]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: 24.787851 ± 11.405171, best_reward: 286.436860 ± 147.444238 in #1\n",
      "\n",
      "[+] Training agent on subject 9...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 1001it [00:02, 467.00it/s, env_step=1000, len=79, loss=1.405, n/ep=0, n/st=10, rew=112.19]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: 313.585106 ± 156.962648, best_reward: 313.585106 ± 156.962648 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 1001it [00:02, 468.95it/s, env_step=2000, len=79, loss=3.238, n/ep=0, n/st=10, rew=95.56]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: 313.585106 ± 160.558441, best_reward: 313.585106 ± 156.962648 in #1\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for subject, (policy, train_collector, test_collector) in enumerate(zip(policies, train_collectors, test_collectors)):\n",
    "    print(f\"\\n[+] Training agent on subject {subject}...\\n\")\n",
    "    result = ts.trainer.offpolicy_trainer(\n",
    "        policy, \n",
    "        train_collector, \n",
    "        test_collector,\n",
    "        max_epoch=num_epochs,\n",
    "        step_per_epoch=num_steps_per_epoch,\n",
    "        step_per_collect=step_per_collect,\n",
    "        episode_per_test=episode_per_test,\n",
    "        batch_size=batch_size,\n",
    "        logger=logger,\n",
    "    )\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'duration': '4.36s',\n",
       "  'train_time/model': '3.71s',\n",
       "  'test_step': 3561,\n",
       "  'test_episode': 15,\n",
       "  'test_time': '0.39s',\n",
       "  'test_speed': '9109.82 step/s',\n",
       "  'best_reward': 66.02701078146637,\n",
       "  'best_result': '66.03 ± 36.12',\n",
       "  'train_step': 2000,\n",
       "  'train_episode': 5,\n",
       "  'train_time/collector': '0.26s',\n",
       "  'train_speed': '504.26 step/s'},\n",
       " {'duration': '4.29s',\n",
       "  'train_time/model': '3.68s',\n",
       "  'test_step': 3561,\n",
       "  'test_episode': 15,\n",
       "  'test_time': '0.36s',\n",
       "  'test_speed': '9952.51 step/s',\n",
       "  'best_reward': 91.11205016798922,\n",
       "  'best_result': '91.11 ± 46.63',\n",
       "  'train_step': 2000,\n",
       "  'train_episode': 5,\n",
       "  'train_time/collector': '0.25s',\n",
       "  'train_speed': '509.19 step/s'},\n",
       " {'duration': '4.59s',\n",
       "  'train_time/model': '3.94s',\n",
       "  'test_step': 3561,\n",
       "  'test_episode': 15,\n",
       "  'test_time': '0.38s',\n",
       "  'test_speed': '9316.81 step/s',\n",
       "  'best_reward': 264.98298045633055,\n",
       "  'best_result': '264.98 ± 135.82',\n",
       "  'train_step': 2000,\n",
       "  'train_episode': 5,\n",
       "  'train_time/collector': '0.26s',\n",
       "  'train_speed': '475.32 step/s'},\n",
       " {'duration': '4.80s',\n",
       "  'train_time/model': '4.11s',\n",
       "  'test_step': 3561,\n",
       "  'test_episode': 15,\n",
       "  'test_time': '0.41s',\n",
       "  'test_speed': '8724.71 step/s',\n",
       "  'best_reward': 166.80094293680594,\n",
       "  'best_result': '166.80 ± 84.88',\n",
       "  'train_step': 2000,\n",
       "  'train_episode': 5,\n",
       "  'train_time/collector': '0.28s',\n",
       "  'train_speed': '455.73 step/s'},\n",
       " {'duration': '4.58s',\n",
       "  'train_time/model': '3.92s',\n",
       "  'test_step': 3561,\n",
       "  'test_episode': 15,\n",
       "  'test_time': '0.38s',\n",
       "  'test_speed': '9358.77 step/s',\n",
       "  'best_reward': 92.85448316464826,\n",
       "  'best_result': '92.85 ± 46.69',\n",
       "  'train_step': 2000,\n",
       "  'train_episode': 5,\n",
       "  'train_time/collector': '0.27s',\n",
       "  'train_speed': '476.79 step/s'},\n",
       " {'duration': '4.68s',\n",
       "  'train_time/model': '4.04s',\n",
       "  'test_step': 3561,\n",
       "  'test_episode': 15,\n",
       "  'test_time': '0.37s',\n",
       "  'test_speed': '9527.95 step/s',\n",
       "  'best_reward': 54.5175263632329,\n",
       "  'best_result': '54.52 ± 30.80',\n",
       "  'train_step': 2000,\n",
       "  'train_episode': 5,\n",
       "  'train_time/collector': '0.26s',\n",
       "  'train_speed': '464.84 step/s'},\n",
       " {'duration': '4.68s',\n",
       "  'train_time/model': '4.03s',\n",
       "  'test_step': 3561,\n",
       "  'test_episode': 15,\n",
       "  'test_time': '0.39s',\n",
       "  'test_speed': '9238.48 step/s',\n",
       "  'best_reward': 59.92726767047716,\n",
       "  'best_result': '59.93 ± 33.37',\n",
       "  'train_step': 2000,\n",
       "  'train_episode': 5,\n",
       "  'train_time/collector': '0.26s',\n",
       "  'train_speed': '465.88 step/s'},\n",
       " {'duration': '4.63s',\n",
       "  'train_time/model': '3.99s',\n",
       "  'test_step': 3561,\n",
       "  'test_episode': 15,\n",
       "  'test_time': '0.38s',\n",
       "  'test_speed': '9361.61 step/s',\n",
       "  'best_reward': 219.13702368121304,\n",
       "  'best_result': '219.14 ± 115.34',\n",
       "  'train_step': 2000,\n",
       "  'train_episode': 5,\n",
       "  'train_time/collector': '0.26s',\n",
       "  'train_speed': '470.50 step/s'},\n",
       " {'duration': '4.70s',\n",
       "  'train_time/model': '4.05s',\n",
       "  'test_step': 3561,\n",
       "  'test_episode': 15,\n",
       "  'test_time': '0.38s',\n",
       "  'test_speed': '9343.59 step/s',\n",
       "  'best_reward': 286.4368596012049,\n",
       "  'best_result': '286.44 ± 147.44',\n",
       "  'train_step': 2000,\n",
       "  'train_episode': 5,\n",
       "  'train_time/collector': '0.27s',\n",
       "  'train_speed': '462.76 step/s'},\n",
       " {'duration': '4.65s',\n",
       "  'train_time/model': '4.02s',\n",
       "  'test_step': 3561,\n",
       "  'test_episode': 15,\n",
       "  'test_time': '0.37s',\n",
       "  'test_speed': '9551.27 step/s',\n",
       "  'best_reward': 313.58510595992414,\n",
       "  'best_result': '313.59 ± 156.96',\n",
       "  'train_step': 2000,\n",
       "  'train_episode': 4,\n",
       "  'train_time/collector': '0.27s',\n",
       "  'train_speed': '467.06 step/s'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_markov_envs = []\n",
    "for subject in subjects:\n",
    "    markov_env = BaseTestEnvironment(\n",
    "        1,\n",
    "        patch_bounding_boxes_per_frame,\n",
    "        patch_centres_per_frame,\n",
    "        speaker_info_per_frame,\n",
    "        foa_centres_per_frame_subjects[subject],\n",
    "        patch_weights_per_frame,\n",
    "        frame_width=320, # from data_utils.py\n",
    "        frame_height=180,\n",
    "    )\n",
    "    test_markov_envs.append(markov_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_envs = []\n",
    "for test_markov_env in test_markov_envs:\n",
    "    num_test_envs = 1 # need set it to 1 (else it doesn't get to the end)\n",
    "\n",
    "    test_envs = ts.env.DummyVectorEnv([lambda: test_markov_env for _ in range(num_test_envs)])\n",
    "    testing_envs.append(test_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n/ep': 10, 'n/st': 1192, 'rews': array([12., 10., 10.,  8.,  8.,  7., 33., 32., 72., 75.]), 'lens': array([ 40,  40,  40,  40,  40,  40, 139, 139, 337, 337]), 'idxs': array([4, 5, 6, 7, 8, 9, 2, 3, 0, 1]), 'rew': 26.7, 'len': 119.2, 'rew_std': 25.127872970070506, 'len_std': 115.45284751793695}\n",
      "{'n/ep': 10, 'n/st': 1192, 'rews': array([ 12.,  12.,  12.,  12.,  12.,  12.,  39.,  41., 102.,  97.]), 'lens': array([ 40,  40,  40,  40,  40,  40, 139, 139, 337, 337]), 'idxs': array([4, 5, 6, 7, 8, 9, 2, 3, 0, 1]), 'rew': 35.1, 'len': 119.2, 'rew_std': 33.99838231445726, 'len_std': 115.45284751793695}\n",
      "{'n/ep': 10, 'n/st': 1192, 'rews': array([ 28.,  28.,  28.,  28.,  27.,  28.,  96.,  99., 241., 243.]), 'lens': array([ 40,  40,  40,  40,  40,  40, 139, 139, 337, 337]), 'idxs': array([4, 5, 6, 7, 8, 9, 2, 3, 0, 1]), 'rew': 84.6, 'len': 119.2, 'rew_std': 83.20120191439544, 'len_std': 115.45284751793695}\n",
      "{'n/ep': 10, 'n/st': 1192, 'rews': array([ 19.,  20.,  19.,  17.,  16.,  15.,  63.,  61., 150., 148.]), 'lens': array([ 40,  40,  40,  40,  40,  40, 139, 139, 337, 337]), 'idxs': array([4, 5, 6, 7, 8, 9, 2, 3, 0, 1]), 'rew': 52.8, 'len': 119.2, 'rew_std': 51.095596679166, 'len_std': 115.45284751793695}\n",
      "{'n/ep': 10, 'n/st': 1192, 'rews': array([10., 10., 10., 11., 13., 14., 39., 38., 89., 91.]), 'lens': array([ 40,  40,  40,  40,  40,  40, 139, 139, 337, 337]), 'idxs': array([4, 5, 6, 7, 8, 9, 2, 3, 0, 1]), 'rew': 32.5, 'len': 119.2, 'rew_std': 30.643922725395324, 'len_std': 115.45284751793695}\n",
      "{'n/ep': 10, 'n/st': 1192, 'rews': array([ 7.,  7.,  7.,  9.,  9.,  8., 26., 23., 62., 64.]), 'lens': array([ 40,  40,  40,  40,  40,  40, 139, 139, 337, 337]), 'idxs': array([4, 5, 6, 7, 8, 9, 2, 3, 0, 1]), 'rew': 22.2, 'len': 119.2, 'rew_std': 21.423351745233514, 'len_std': 115.45284751793695}\n",
      "{'n/ep': 10, 'n/st': 1192, 'rews': array([ 6.,  6.,  7.,  9.,  9.,  7., 24., 24., 59., 62.]), 'lens': array([ 40,  40,  40,  40,  40,  40, 139, 139, 337, 337]), 'idxs': array([4, 5, 6, 7, 8, 9, 2, 3, 0, 1]), 'rew': 21.3, 'len': 119.2, 'rew_std': 20.669059001318857, 'len_std': 115.45284751793695}\n",
      "{'n/ep': 10, 'n/st': 1192, 'rews': array([ 23.,  23.,  24.,  22.,  24.,  23.,  81.,  79., 196., 192.]), 'lens': array([ 40,  40,  40,  40,  40,  40, 139, 139, 337, 337]), 'idxs': array([4, 5, 6, 7, 8, 9, 2, 3, 0, 1]), 'rew': 68.7, 'len': 119.2, 'rew_std': 66.41392926186494, 'len_std': 115.45284751793695}\n",
      "{'n/ep': 10, 'n/st': 1192, 'rews': array([ 4.,  4.,  3.,  4.,  3.,  2., 10., 11., 22., 27.]), 'lens': array([ 40,  40,  40,  40,  40,  40, 139, 139, 337, 337]), 'idxs': array([4, 5, 6, 7, 8, 9, 2, 3, 0, 1]), 'rew': 9.0, 'len': 119.2, 'rew_std': 8.330666239863412, 'len_std': 115.45284751793695}\n",
      "{'n/ep': 10, 'n/st': 1192, 'rews': array([ 33.,  33.,  32.,  32.,  32.,  34., 112., 114., 276., 275.]), 'lens': array([ 40,  40,  40,  40,  40,  40, 139, 139, 337, 337]), 'idxs': array([4, 5, 6, 7, 8, 9, 2, 3, 0, 1]), 'rew': 97.3, 'len': 119.2, 'rew_std': 94.37907607091732, 'len_std': 115.45284751793695}\n"
     ]
    }
   ],
   "source": [
    "#! notice that the policy is unchanged\n",
    "for policy, test_envs in zip(policies, testing_envs):\n",
    "    policy.eval()\n",
    "    policy.set_eps(0.05)\n",
    "\n",
    "    collector = ts.data.Collector(policy, test_envs)\n",
    "    # should be the same values 3 times (if not, there's a problem)\n",
    "    print(collector.collect(n_episode=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
