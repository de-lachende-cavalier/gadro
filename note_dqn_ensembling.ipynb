{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN with Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a personal curiosity, I'd like to try to squeeze as much performance as possible from the initial DQN application.\n",
    "\n",
    "I intend to do so using ensembling: I want to train multiple agents with the same video and different subjects and then use **majority-voting** at test time, to see if we can get any performance improvements, even with an extremely simple approach!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tianshou as ts \n",
    "from tianshou.utils import TensorboardLogger\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johndoe/Desktop/uni/affectivecompute/project/lib/python3.11/site-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "from utils_preprocess import compute_frame_features, compute_foa_features\n",
    "\n",
    "from env_base import BaseEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and environment initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_filename = \"001\"\n",
    "mat_filename = vid_filename + \".mat\"\n",
    "\n",
    "n_subjects = 10 # pick the first ten subjects (out of 39)\n",
    "subjects = [n for n in range(n_subjects)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_bounding_boxes_per_frame, patch_centres_per_frame, speaker_info_per_frame = compute_frame_features(\n",
    "    vid_filename\n",
    ")\n",
    "\n",
    "foa_centres_per_frame_per_subject, patch_weights_per_frame = compute_foa_features(\n",
    "    mat_filename, patch_centres_per_frame\n",
    ")\n",
    "\n",
    "foa_centres_per_frame_subjects = [\n",
    "    [frame[target_subject] for frame in foa_centres_per_frame_per_subject] \n",
    "    for target_subject in subjects\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_envs = []\n",
    "for subject in subjects:\n",
    "    markov_env = BaseEnvironment(\n",
    "        1,\n",
    "        patch_bounding_boxes_per_frame,\n",
    "        patch_centres_per_frame,\n",
    "        speaker_info_per_frame,\n",
    "        foa_centres_per_frame_subjects[subject],\n",
    "        patch_weights_per_frame,\n",
    "        frame_width=320, # from data_utils.py\n",
    "        frame_height=180,\n",
    "    )\n",
    "    markov_envs.append(markov_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_envs = []\n",
    "all_test_envs = []\n",
    "for markov_env in markov_envs:\n",
    "    num_train_envs = 5\n",
    "    num_test_envs = 10\n",
    "\n",
    "    train_envs = ts.env.DummyVectorEnv([lambda: markov_env for _ in range(num_train_envs)])\n",
    "    all_train_envs.append(train_envs)\n",
    "\n",
    "    test_envs = ts.env.DummyVectorEnv([lambda: markov_env for _ in range(num_test_envs)])\n",
    "    all_test_envs.append(test_envs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's construct the network.\n",
    "\n",
    "The biggest headache comes from the observations: they're quite complex. So, we build multiple networks, each processing a part of an observation and combining their outputs in the end!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, observation_space, action_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_patches = observation_space['patch_centres'].shape[0]\n",
    "\n",
    "        # network for patch_centres\n",
    "        self.patch_centres_net = nn.Sequential(\n",
    "            nn.Linear(np.prod(observation_space['patch_centres'].shape), 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # network for patch_bounding_boxes\n",
    "        self.patch_bboxes_net = nn.Sequential(\n",
    "            nn.Linear(np.prod(observation_space['patch_bounding_boxes'].shape), 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # network for speaker_info\n",
    "        self.speaker_info_net = nn.Sequential(\n",
    "            nn.Linear(np.prod(observation_space['speaker_info'].shape), 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # combining the outputs of all networks\n",
    "        self.combined_net = nn.Sequential(\n",
    "            nn.Linear(64 + 64 + 32, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, np.prod(action_shape))\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, state=None, info={}):\n",
    "        patch_centres = torch.tensor(obs['patch_centres'], dtype=torch.float32)\n",
    "        patch_bboxes = torch.tensor(obs['patch_bounding_boxes'], dtype=torch.float32)\n",
    "        speaker_info = torch.tensor(obs['speaker_info'], dtype=torch.float32)\n",
    "\n",
    "        patch_centres = patch_centres.view(patch_centres.size(0), -1)\n",
    "        patch_bboxes = patch_bboxes.view(patch_bboxes.size(0), -1)\n",
    "        speaker_info = speaker_info.view(speaker_info.size(0), -1)\n",
    "\n",
    "        # pass through respective networks\n",
    "        patch_centres_out = self.patch_centres_net(patch_centres)\n",
    "        patch_bboxes_out = self.patch_bboxes_net(patch_bboxes)\n",
    "        speaker_info_out = self.speaker_info_net(speaker_info)\n",
    "\n",
    "        # combine outputs\n",
    "        combined = torch.cat([patch_centres_out, patch_bboxes_out, speaker_info_out], dim=1)\n",
    "\n",
    "        logits = self.combined_net(combined)\n",
    "\n",
    "        return logits, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = []\n",
    "optims = []\n",
    "for markov_env in markov_envs:\n",
    "    state_shape = markov_env.observation_space\n",
    "    action_shape = markov_env.action_space.n\n",
    "\n",
    "    net = Net(state_shape, action_shape)\n",
    "    nets.append(net)\n",
    "\n",
    "    optim = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "    optims.append(optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to set up the policy, which is readily done in Tianshou."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = []\n",
    "for net, optim in zip(nets, optims):\n",
    "    policy = ts.policy.DQNPolicy(\n",
    "        model=net, \n",
    "        optim=optim, \n",
    "        discount_factor=0.99,\n",
    "        estimation_step=1,\n",
    "        target_update_freq=50\n",
    "    )\n",
    "    policies.append(policies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to set up the collectors, i.e., the objects that will be interacting with the environment according to the above policy and collect the generated data.\n",
    "\n",
    "In classical DQN fashion, we store the data in a replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_collectors = []\n",
    "test_collectors = []\n",
    "for policy in policies:\n",
    "    train_collector = ts.data.Collector(policy, train_envs, ts.data.VectorReplayBuffer(1000, num_train_envs))\n",
    "    train_collectors.append(train_collector)\n",
    "\n",
    "    test_collector = ts.data.Collector(policy, test_envs)\n",
    "    test_collectors.append(test_collector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "num_steps_per_epoch = 2000\n",
    "step_per_collect = 10\n",
    "episode_per_test = 5\n",
    "batch_size = 30 # one second of data (videos are at 30FPS)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%d%m%Y-%H%M%S\")\n",
    "log_path = os.path.join(\"logs\", \"dqn\", \"base\", f\"video_{vid_filename}\", f\"ensemble_{n_subjects}\", timestamp)\n",
    "writer = SummaryWriter(log_path)\n",
    "logger = TensorboardLogger(writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 3001it [00:06, 445.45it/s, env_step=3000, len=120, loss=1.504, n/ep=2, n/st=10, rew=56.35]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: 153.505796 ± 126.639527, best_reward: 154.035783 ± 125.348117 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 3001it [00:06, 454.86it/s, env_step=6000, len=120, loss=1.710, n/ep=2, n/st=10, rew=47.47]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: 153.505796 ± 124.430770, best_reward: 154.035783 ± 125.348117 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 3001it [00:06, 445.62it/s, env_step=9000, len=120, loss=2.095, n/ep=2, n/st=10, rew=49.39]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3: test_reward: 153.505796 ± 124.688519, best_reward: 154.035783 ± 125.348117 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 3001it [00:06, 442.91it/s, env_step=12000, len=120, loss=2.792, n/ep=2, n/st=10, rew=53.12]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4: test_reward: 153.505796 ± 119.897613, best_reward: 154.035783 ± 125.348117 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 3001it [00:06, 449.41it/s, env_step=15000, len=120, loss=2.323, n/ep=2, n/st=10, rew=56.92]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5: test_reward: 153.505796 ± 126.568137, best_reward: 154.035783 ± 125.348117 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6: 3001it [00:06, 453.59it/s, env_step=18000, len=120, loss=3.602, n/ep=2, n/st=10, rew=46.87]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #6: test_reward: 153.505796 ± 126.425746, best_reward: 154.035783 ± 125.348117 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #7: 3001it [00:06, 452.84it/s, env_step=21000, len=120, loss=4.078, n/ep=2, n/st=10, rew=53.51]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #7: test_reward: 153.505796 ± 122.082291, best_reward: 154.035783 ± 125.348117 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #8: 3001it [00:07, 380.55it/s, env_step=24000, len=120, loss=2.532, n/ep=2, n/st=10, rew=52.90]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #8: test_reward: 153.505796 ± 122.433377, best_reward: 154.035783 ± 125.348117 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #9: 3001it [00:06, 451.10it/s, env_step=27000, len=120, loss=3.774, n/ep=2, n/st=10, rew=46.32]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #9: test_reward: 153.505796 ± 120.639651, best_reward: 154.035783 ± 125.348117 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #10: 3001it [00:06, 456.66it/s, env_step=30000, len=120, loss=4.525, n/ep=2, n/st=10, rew=52.45]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #10: test_reward: 153.505796 ± 124.869452, best_reward: 154.035783 ± 125.348117 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #11: 3001it [00:06, 454.32it/s, env_step=33000, len=120, loss=2.671, n/ep=2, n/st=10, rew=54.12]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #11: test_reward: 153.505796 ± 124.981207, best_reward: 154.035783 ± 125.348117 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #12: 3001it [00:06, 453.76it/s, env_step=36000, len=120, loss=4.172, n/ep=2, n/st=10, rew=50.39]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #12: test_reward: 153.505796 ± 123.342576, best_reward: 154.035783 ± 125.348117 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #13: 3001it [00:06, 452.23it/s, env_step=39000, len=120, loss=3.957, n/ep=2, n/st=10, rew=49.02]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #13: test_reward: 153.505796 ± 116.355178, best_reward: 154.035783 ± 125.348117 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #14: 3001it [00:06, 449.24it/s, env_step=42000, len=120, loss=4.048, n/ep=2, n/st=10, rew=49.77]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #14: test_reward: 153.505796 ± 119.584744, best_reward: 154.035783 ± 125.348117 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #15: 3001it [00:06, 451.30it/s, env_step=45000, len=120, loss=2.976, n/ep=2, n/st=10, rew=64.22]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #15: test_reward: 153.505796 ± 120.258150, best_reward: 154.035783 ± 125.348117 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #16: 3001it [00:06, 471.52it/s, env_step=48000, len=120, loss=3.944, n/ep=2, n/st=10, rew=49.51]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #16: test_reward: 153.505796 ± 130.642619, best_reward: 154.035783 ± 125.348117 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #17: 3001it [00:06, 472.22it/s, env_step=51000, len=120, loss=3.543, n/ep=2, n/st=10, rew=53.90]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #17: test_reward: 153.505796 ± 126.683213, best_reward: 154.035783 ± 125.348117 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #18: 3001it [00:06, 453.12it/s, env_step=54000, len=120, loss=2.790, n/ep=2, n/st=10, rew=52.41]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #18: test_reward: 153.505796 ± 125.293721, best_reward: 154.035783 ± 125.348117 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #19: 3001it [00:06, 454.56it/s, env_step=57000, len=120, loss=4.958, n/ep=2, n/st=10, rew=55.25]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #19: test_reward: 153.505796 ± 115.376002, best_reward: 154.035783 ± 125.348117 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #20: 3001it [00:06, 454.52it/s, env_step=60000, len=120, loss=2.159, n/ep=2, n/st=10, rew=53.47]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #20: test_reward: 153.505796 ± 127.634869, best_reward: 154.035783 ± 125.348117 in #0\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for policy, train_collector, test_collector in zip(policies, train_collectors, test_collectors):\n",
    "    result = ts.trainer.offpolicy_trainer(\n",
    "        policy, \n",
    "        train_collector, \n",
    "        test_collector,\n",
    "        max_epoch=num_epochs,\n",
    "        step_per_epoch=num_steps_per_epoch,\n",
    "        step_per_collect=step_per_collect,\n",
    "        episode_per_test=episode_per_test,\n",
    "        batch_size=batch_size,\n",
    "        logger=logger,\n",
    "    )\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'duration': '138.74s',\n",
       " 'train_time/model': '125.34s',\n",
       " 'test_step': 37779,\n",
       " 'test_episode': 105,\n",
       " 'test_time': '5.06s',\n",
       " 'test_speed': '7465.91 step/s',\n",
       " 'best_reward': 154.03578341935645,\n",
       " 'best_result': '154.04 ± 125.35',\n",
       " 'train_step': 60000,\n",
       " 'train_episode': 200,\n",
       " 'train_time/collector': '8.34s',\n",
       " 'train_speed': '448.83 step/s'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n/ep': 10,\n",
       " 'n/st': 3000,\n",
       " 'rews': array([ 55.09495263,  50.14317183,  53.25710873,  58.55555754,\n",
       "         59.01095125,  47.6988985 , 220.29657369,  52.25722118,\n",
       "        368.95651349, 316.79558103]),\n",
       " 'lens': array([120, 120, 120, 120, 120, 120, 510, 150, 810, 810]),\n",
       " 'idxs': array([3, 4, 3, 4, 3, 4, 2, 4, 0, 1]),\n",
       " 'rew': 128.20665298717904,\n",
       " 'len': 300.0,\n",
       " 'rew_std': 118.72347264807837,\n",
       " 'len_std': 279.4995527724508}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for policy, train_envs in zip(policies, all_train_envs):\n",
    "    policy.eval()\n",
    "    policy.set_eps(0.05)\n",
    "\n",
    "    collector = ts.data.Collector(policy, train_envs)\n",
    "    print(collector.collect(n_episode=10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
