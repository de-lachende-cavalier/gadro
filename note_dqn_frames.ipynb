{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN again, but with more information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before abandoning DQN for a more full-featured approach, I want to try to apply it to a more informative environment: one that feeds the dynamic and the video frames, along with the patch data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tianshou as ts \n",
    "from tianshou.utils import TensorboardLogger\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johndoe/Desktop/uni/affectivecompute/project/lib/python3.11/site-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "from utils_preprocess import compute_frame_features, compute_foa_features\n",
    "from utils_data import get_frames_by_type, get_feature_frames\n",
    "\n",
    "from env_frames import FramesEnvironment\n",
    "from env_frames_test import FramesTestEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_filename = \"012\"\n",
    "mat_filename = vid_filename + \".mat\"\n",
    "target_subject = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_frames = get_frames_by_type(\"frames\", vid_filename)\n",
    "dynamics_frames = get_frames_by_type(\"dynamic\", vid_filename)\n",
    "patches_frames, _ = get_feature_frames(vid_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to normalise frames before feeding them into the model\n",
    "for i in range(len(video_frames)):\n",
    "    # broadcasting will do the rest...\n",
    "    video_frames[i] = video_frames[i] / 255.0\n",
    "    dynamics_frames[i] = dynamics_frames[i] / 255.0\n",
    "    patches_frames[i] = patches_frames[i] / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_bounding_boxes_per_frame, patch_centres_per_frame, speaker_info_per_frame = compute_frame_features(\n",
    "    vid_filename\n",
    ")\n",
    "\n",
    "foa_centres_per_frame_per_subject, patch_weights_per_frame = compute_foa_features(\n",
    "    mat_filename, patch_centres_per_frame\n",
    ")\n",
    "foa_centres_per_frame = [frame[target_subject] for frame in foa_centres_per_frame_per_subject]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_env = FramesEnvironment(\n",
    "    1,\n",
    "    video_frames,\n",
    "    dynamics_frames,\n",
    "    patches_frames,\n",
    "    patch_bounding_boxes_per_frame,\n",
    "    patch_centres_per_frame,\n",
    "    speaker_info_per_frame,\n",
    "    foa_centres_per_frame,\n",
    "    patch_weights_per_frame,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.observation_space.sample(), env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_envs = 5\n",
    "num_test_envs = 10\n",
    "\n",
    "train_envs = ts.env.DummyVectorEnv([lambda: markov_env for _ in range(num_train_envs)])\n",
    "test_envs = ts.env.DummyVectorEnv([lambda: markov_env for _ in range(num_test_envs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, observation_space, action_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_patches = observation_space['patch_centres'].shape[0]\n",
    "\n",
    "        self.rgb_frame_net = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=5, stride=2, padding=2),  # [3, 180, 320]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # [16, 45, 80]\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=2, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # [32, 11, 20]\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),  # [64, 6, 10]\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 6 * 10, 64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.bitmap_frame_net = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=2, padding=2),  # [1, 180, 320]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # [16, 45, 80]\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=2, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # [32, 11, 20]\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),  # [64, 6, 10]\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 6 * 10, 64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # network for patch_centres\n",
    "        self.patch_centres_net = nn.Sequential(\n",
    "            nn.Linear(np.prod(observation_space['patch_centres'].shape), 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # network for speaker_info\n",
    "        self.speaker_info_net = nn.Sequential(\n",
    "            nn.Linear(np.prod(observation_space['speaker_info'].shape), 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # combining the outputs of all networks\n",
    "        self.combined_net = nn.Sequential(\n",
    "            nn.Linear(64 + 32 + 64 + 64 + 64, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, np.prod(action_shape))\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, state=None, info={}):\n",
    "        patch_centres = torch.tensor(obs['patch_centres'], dtype=torch.float32)\n",
    "        speaker_info = torch.tensor(obs['speaker_info'], dtype=torch.float32)\n",
    "        \n",
    "        patch_centres = patch_centres.view(patch_centres.size(0), -1)\n",
    "        speaker_info = speaker_info.view(speaker_info.size(0), -1)\n",
    "\n",
    "        video_frame = torch.tensor(obs['video_frames'], dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "        dynamics_frame = torch.tensor(obs['dynamics_frames'], dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "        patches_frame = torch.tensor(obs['patches_frames'], dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "        # pass through respective networks\n",
    "        patch_centres_out = self.patch_centres_net(patch_centres)\n",
    "        speaker_info_out = self.speaker_info_net(speaker_info)\n",
    "\n",
    "        video_frame_out = self.rgb_frame_net(video_frame)\n",
    "        dynamics_frame_out = self.rgb_frame_net(dynamics_frame)\n",
    "        patches_frame_out = self.bitmap_frame_net(patches_frame)\n",
    "\n",
    "        # combine outputs\n",
    "        combined = torch.cat([patch_centres_out, speaker_info_out, video_frame_out, dynamics_frame_out, patches_frame_out], dim=1)\n",
    "\n",
    "        logits = self.combined_net(combined)\n",
    "\n",
    "        return logits, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_shape = markov_env.observation_space\n",
    "action_shape = markov_env.action_space.n\n",
    "\n",
    "net = Net(state_shape, action_shape)\n",
    "optim = torch.optim.Adam(net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (rgb_frame_net): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(16, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Flatten(start_dim=1, end_dim=-1)\n",
       "    (9): Linear(in_features=3840, out_features=64, bias=True)\n",
       "    (10): ReLU(inplace=True)\n",
       "  )\n",
       "  (bitmap_frame_net): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(16, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Flatten(start_dim=1, end_dim=-1)\n",
       "    (9): Linear(in_features=3840, out_features=64, bias=True)\n",
       "    (10): ReLU(inplace=True)\n",
       "  )\n",
       "  (patch_centres_net): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=64, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (speaker_info_net): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (combined_net): Sequential(\n",
       "    (0): Linear(in_features=288, out_features=128, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=128, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike in the simpler DQN case, I need to significanly reduce the memory footprint of my code to avoid crashing the jupyter kernel and running out of RAM.\n",
    "\n",
    "Naturally, if you're on a more powerful machine, feel free to increase the various memory-intensive parameters.\n",
    "\n",
    "(I could also consider a streaming approach to loading the various frames, seeing as they're the main culprits behind the kernel crashes, but that would require quite the engineering effort and would be quite out of scope.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = ts.policy.DQNPolicy(\n",
    "    model=net, \n",
    "    optim=optim, \n",
    "    discount_factor=0.99,\n",
    "    estimation_step=1,\n",
    "    target_update_freq=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_collector = ts.data.Collector(policy, train_envs, ts.data.VectorReplayBuffer(100, num_train_envs))\n",
    "\n",
    "test_collector = ts.data.Collector(policy, test_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "num_steps_per_epoch = 50\n",
    "step_per_collect = 5 \n",
    "episode_per_test = 2\n",
    "batch_size = 10 \n",
    "\n",
    "timestamp = datetime.now().strftime(\"%d%m%Y-%H%M%S\")\n",
    "log_path = os.path.join(\"logs\", \"dqn\", \"frames\", f\"video_{vid_filename}\", f\"subject_{target_subject}\", timestamp)\n",
    "writer = SummaryWriter(log_path)\n",
    "logger = TensorboardLogger(writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 51it [00:06,  7.69it/s, env_step=50, len=0, loss=0.592, n/ep=0, n/st=5, rew=0.00]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: 55.022509 ± 1.961869, best_reward: 55.022509 ± 3.044663 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 51it [00:06,  7.78it/s, env_step=100, len=0, loss=0.529, n/ep=0, n/st=5, rew=0.00]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: 55.022509 ± 5.037302, best_reward: 55.022509 ± 3.044663 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 51it [00:06,  7.77it/s, env_step=150, len=0, loss=0.449, n/ep=0, n/st=5, rew=0.00]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3: test_reward: 55.022509 ± 8.124387, best_reward: 55.022509 ± 3.044663 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 51it [00:06,  7.76it/s, env_step=200, len=0, loss=0.427, n/ep=0, n/st=5, rew=0.00]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4: test_reward: 55.022509 ± 3.746921, best_reward: 55.022509 ± 3.044663 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 51it [00:06,  7.79it/s, env_step=250, len=0, loss=0.352, n/ep=0, n/st=5, rew=0.00]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5: test_reward: 55.022509 ± 6.822969, best_reward: 55.022509 ± 3.044663 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6: 51it [00:06,  7.60it/s, env_step=300, len=0, loss=0.334, n/ep=0, n/st=5, rew=0.00]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #6: test_reward: 55.022509 ± 5.836692, best_reward: 55.022509 ± 3.044663 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #7: 51it [00:06,  7.41it/s, env_step=350, len=0, loss=0.389, n/ep=0, n/st=5, rew=0.00]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #7: test_reward: 55.022509 ± 0.484511, best_reward: 55.022509 ± 3.044663 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #8: 51it [00:07,  7.28it/s, env_step=400, len=0, loss=0.397, n/ep=0, n/st=5, rew=0.00]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #8: test_reward: 55.022509 ± 0.274288, best_reward: 55.022509 ± 3.044663 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #9: 51it [00:06,  7.33it/s, env_step=450, len=0, loss=0.340, n/ep=0, n/st=5, rew=0.00]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #9: test_reward: 55.022509 ± 2.407658, best_reward: 55.022509 ± 3.044663 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #10: 51it [00:07,  7.07it/s, env_step=500, len=0, loss=0.280, n/ep=0, n/st=5, rew=0.00]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #10: test_reward: 55.022509 ± 9.313679, best_reward: 55.022509 ± 3.044663 in #0\n"
     ]
    }
   ],
   "source": [
    "result = ts.trainer.offpolicy_trainer(\n",
    "    policy, \n",
    "    train_collector, \n",
    "    test_collector,\n",
    "    max_epoch=num_epochs,\n",
    "    step_per_epoch=num_steps_per_epoch,\n",
    "    step_per_collect=step_per_collect,\n",
    "    episode_per_test=episode_per_test,\n",
    "    batch_size=batch_size,\n",
    "    logger=logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'duration': '93.75s',\n",
       " 'train_time/model': '65.78s',\n",
       " 'test_step': 4356,\n",
       " 'test_episode': 22,\n",
       " 'test_time': '25.95s',\n",
       " 'test_speed': '167.89 step/s',\n",
       " 'best_reward': 55.022508984555316,\n",
       " 'best_result': '55.02 ± 3.04',\n",
       " 'train_step': 500,\n",
       " 'train_episode': 0,\n",
       " 'train_time/collector': '2.02s',\n",
       " 'train_speed': '7.37 step/s'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_markov_env = FramesTestEnvironment(\n",
    "    1,\n",
    "    video_frames,\n",
    "    dynamics_frames,\n",
    "    patches_frames,\n",
    "    patch_bounding_boxes_per_frame,\n",
    "    patch_centres_per_frame,\n",
    "    speaker_info_per_frame,\n",
    "    foa_centres_per_frame,\n",
    "    patch_weights_per_frame,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_envs = 1\n",
    "\n",
    "testing_envs = ts.env.DummyVectorEnv([lambda: test_markov_env for _ in range(num_test_envs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m collector \u001b[38;5;241m=\u001b[39m ts\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mCollector(policy, testing_envs)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# should be the same values 3 times (if not, there's a problem)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mcollector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_episode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/uni/affectivecompute/project/lib/python3.11/site-packages/tianshou/data/collector.py:279\u001b[0m, in \u001b[0;36mCollector.collect\u001b[0;34m(self, n_step, n_episode, random, render, no_grad, gym_reset_kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m no_grad:\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# faster than retain_grad version\u001b[39;00m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;66;03m# self.data.obs will be used by agent to get result\u001b[39;00m\n\u001b[0;32m--> 279\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    281\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, last_state)\n",
      "File \u001b[0;32m~/Desktop/uni/affectivecompute/project/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/uni/affectivecompute/project/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/uni/affectivecompute/project/lib/python3.11/site-packages/tianshou/policy/modelfree/dqn.py:160\u001b[0m, in \u001b[0;36mDQNPolicy.forward\u001b[0;34m(self, batch, state, model, input, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m obs \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;28minput\u001b[39m]\n\u001b[1;32m    159\u001b[0m obs_next \u001b[38;5;241m=\u001b[39m obs\u001b[38;5;241m.\u001b[39mobs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m obs\n\u001b[0;32m--> 160\u001b[0m logits, hidden \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_next\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_q_value(logits, \u001b[38;5;28mgetattr\u001b[39m(obs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_action_num\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Desktop/uni/affectivecompute/project/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/uni/affectivecompute/project/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 75\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, obs, state, info)\u001b[0m\n\u001b[1;32m     72\u001b[0m patch_centres_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_centres_net(patch_centres)\n\u001b[1;32m     73\u001b[0m speaker_info_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeaker_info_net(speaker_info)\n\u001b[0;32m---> 75\u001b[0m video_frame_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrgb_frame_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m dynamics_frame_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrgb_frame_net(dynamics_frame)\n\u001b[1;32m     77\u001b[0m patches_frame_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbitmap_frame_net(patches_frame)\n",
      "File \u001b[0;32m~/Desktop/uni/affectivecompute/project/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/uni/affectivecompute/project/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/uni/affectivecompute/project/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/uni/affectivecompute/project/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/uni/affectivecompute/project/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/uni/affectivecompute/project/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "policy.eval()\n",
    "policy.set_eps(0.05)\n",
    "\n",
    "collector = ts.data.Collector(policy, testing_envs)\n",
    "# should be the same values 3 times (if not, there's a problem)\n",
    "collector.collect(n_episode=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 4585), started 0:02:03 ago. (Use '!kill 4585' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7f83138a80e6cd5b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7f83138a80e6cd5b\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/dqn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
