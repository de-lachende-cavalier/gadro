{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A first stab: DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[DQN](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) is a classical RL algorithm which should provide a nice baseline for further work.\n",
    "\n",
    "Classical RL techniques woul probably not work very well without further feature engineering, because the current state space is quite large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tianshou as ts \n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johndoe/Desktop/uni/affectivecompute/project/lib/python3.11/site-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "from markov_gaze_env import MarkovGazeEnv\n",
    "from preprocess_utils import compute_frame_features, compute_foa_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and environment initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_filename = \"012\"\n",
    "mat_filename = vid_filename + \".mat\"\n",
    "target_subject = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_bounding_boxes, patch_centres, speaker_info = compute_frame_features(\n",
    "    vid_filename\n",
    ")\n",
    "\n",
    "foa_centres, patch_weights_per_frame = compute_foa_features(\n",
    "    mat_filename, patch_centres\n",
    ")\n",
    "foa_centres_single_subject = [frame[target_subject] for frame in foa_centres]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MarkovGazeEnv(\n",
    "    patch_bounding_boxes,\n",
    "    patch_centres,\n",
    "    speaker_info,\n",
    "    foa_centres_single_subject,\n",
    "    patch_weights_per_frame,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.observation_space.sample(), env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For efficiency, it's a good idea to set up some vectorized environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_envs = 5\n",
    "num_test_envs = 10\n",
    "\n",
    "train_envs = ts.env.DummyVectorEnv([lambda: env for _ in range(num_train_envs)])\n",
    "test_envs = ts.env.DummyVectorEnv([lambda: env for _ in range(num_test_envs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's construct the network.\n",
    "\n",
    "The biggest headache comes from the observations: they're quite complex. So, we build multiple networks, each processing a part of an observation and combining their outputs in the end!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, observation_space, action_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_patches = observation_space['patch_centres'].shape[0]\n",
    "\n",
    "        # network for patch_centres\n",
    "        self.patch_centres_net = nn.Sequential(\n",
    "            nn.Linear(np.prod(observation_space['patch_centres'].shape), 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # network for patch_bounding_boxes\n",
    "        self.patch_bboxes_net = nn.Sequential(\n",
    "            nn.Linear(np.prod(observation_space['patch_bounding_boxes'].shape), 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # network for speaker_info\n",
    "        self.speaker_info_net = nn.Sequential(\n",
    "            nn.Linear(np.prod(observation_space['speaker_info'].shape), 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # combining the outputs of all networks\n",
    "        self.combined_net = nn.Sequential(\n",
    "            nn.Linear(64 + 64 + 32, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, np.prod(action_shape))\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, state=None, info={}):\n",
    "        patch_centres = torch.tensor(obs['patch_centres'], dtype=torch.float32)\n",
    "        patch_bboxes = torch.tensor(obs['patch_bounding_boxes'], dtype=torch.float32)\n",
    "        speaker_info = torch.tensor(obs['speaker_info'], dtype=torch.float32)\n",
    "\n",
    "        patch_centres = patch_centres.view(patch_centres.size(0), -1)\n",
    "        patch_bboxes = patch_bboxes.view(patch_bboxes.size(0), -1)\n",
    "        speaker_info = speaker_info.view(speaker_info.size(0), -1)\n",
    "\n",
    "        # pass through respective networks\n",
    "        patch_centres_out = self.patch_centres_net(patch_centres)\n",
    "        patch_bboxes_out = self.patch_bboxes_net(patch_bboxes)\n",
    "        speaker_info_out = self.speaker_info_net(speaker_info)\n",
    "\n",
    "        # combine outputs\n",
    "        combined = torch.cat([patch_centres_out, patch_bboxes_out, speaker_info_out], dim=1)\n",
    "\n",
    "        logits = self.combined_net(combined)\n",
    "\n",
    "        return logits, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_shape = env.observation_space\n",
    "action_shape = env.action_space.n\n",
    "\n",
    "net = Net(state_shape, action_shape)\n",
    "optim = torch.optim.Adam(net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (patch_centres_net): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=64, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (patch_bboxes_net): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (speaker_info_net): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (combined_net): Sequential(\n",
       "    (0): Linear(in_features=160, out_features=128, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=128, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to set up the policy, which is readily done in Tianshou."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = ts.policy.DQNPolicy(\n",
    "    model=net, \n",
    "    optim=optim, \n",
    "    discount_factor=0.99,\n",
    "    estimation_step=3,\n",
    "    target_update_freq=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to set up the collectors, i.e., the objects that will be interacting with the environment according to the above policy and collect the generated data.\n",
    "\n",
    "In classical DQN fashion, we store the data in a replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_collector = ts.data.Collector(policy, train_envs, ts.data.VectorReplayBuffer(1000, num_train_envs))\n",
    "\n",
    "test_collector = ts.data.Collector(policy, test_envs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "num_steps_per_epoch = 1000\n",
    "step_per_collect = 10 # update the Q-values at each step\n",
    "episode_per_test = 5 # use 5 episodes to test the policy\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 1001it [00:02, 450.86it/s, env_step=1000, len=79, loss=0.319, n/ep=0, n/st=10, rew=7.00]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: 26.600000 ± 12.419340, best_reward: 26.600000 ± 14.650597 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 1001it [00:02, 491.01it/s, env_step=2000, len=79, loss=0.567, n/ep=0, n/st=10, rew=4.00]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: 26.400000 ± 13.908271, best_reward: 26.600000 ± 14.650597 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 1001it [00:02, 492.37it/s, env_step=3000, len=79, loss=0.349, n/ep=0, n/st=10, rew=9.00]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3: test_reward: 26.400000 ± 14.934524, best_reward: 26.600000 ± 14.650597 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 1001it [00:02, 497.65it/s, env_step=4000, len=79, loss=0.701, n/ep=0, n/st=10, rew=7.00]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4: test_reward: 26.400000 ± 15.213152, best_reward: 26.600000 ± 14.650597 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 1001it [00:02, 488.10it/s, env_step=5000, len=79, loss=0.403, n/ep=0, n/st=10, rew=8.00]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5: test_reward: 26.400000 ± 14.079773, best_reward: 26.600000 ± 14.650597 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6: 1001it [00:02, 494.19it/s, env_step=6000, len=79, loss=1.308, n/ep=0, n/st=10, rew=9.00]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #6: test_reward: 26.400000 ± 15.409088, best_reward: 26.600000 ± 14.650597 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #7: 1001it [00:02, 495.55it/s, env_step=7000, len=79, loss=1.071, n/ep=0, n/st=10, rew=7.00]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #7: test_reward: 26.400000 ± 13.734628, best_reward: 26.600000 ± 14.650597 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #8: 1001it [00:02, 494.18it/s, env_step=8000, len=79, loss=2.441, n/ep=0, n/st=10, rew=8.00]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #8: test_reward: 26.400000 ± 14.827002, best_reward: 26.600000 ± 14.650597 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #9: 1001it [00:02, 489.81it/s, env_step=9000, len=79, loss=1.202, n/ep=0, n/st=10, rew=5.00]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #9: test_reward: 26.400000 ± 14.813507, best_reward: 26.600000 ± 14.650597 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #10: 1001it [00:02, 499.02it/s, env_step=10000, len=79, loss=2.410, n/ep=0, n/st=10, rew=9.00]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #10: test_reward: 26.400000 ± 11.377170, best_reward: 26.600000 ± 14.650597 in #0\n"
     ]
    }
   ],
   "source": [
    "result = ts.trainer.offpolicy_trainer(\n",
    "    policy, \n",
    "    train_collector, \n",
    "    test_collector,\n",
    "    max_epoch=num_epochs,\n",
    "    step_per_epoch=num_steps_per_epoch,\n",
    "    step_per_collect=step_per_collect,\n",
    "    episode_per_test=episode_per_test,\n",
    "    batch_size=batch_size,\n",
    "    test_in_train=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'duration': '22.12s',\n",
       " 'train_time/model': '18.96s',\n",
       " 'test_step': 13057,\n",
       " 'test_episode': 55,\n",
       " 'test_time': '1.60s',\n",
       " 'test_speed': '8174.61 step/s',\n",
       " 'best_reward': 26.6,\n",
       " 'best_result': '26.60 ± 14.65',\n",
       " 'train_step': 10000,\n",
       " 'train_episode': 20,\n",
       " 'train_time/collector': '1.56s',\n",
       " 'train_speed': '487.24 step/s'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
