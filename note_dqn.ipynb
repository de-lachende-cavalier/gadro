{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A first stab: DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[DQN](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) is a classical RL algorithm which should provide a nice baseline for further work.\n",
    "\n",
    "Classical RL techniques woul probably not work very well without further feature engineering, because the current state space is quite large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tianshou as ts \n",
    "from tianshou.utils import TensorboardLogger\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johndoe/Desktop/uni/affectivenatural/project/lib/python3.11/site-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "from utils_preprocess import compute_frame_features, compute_foa_features\n",
    "\n",
    "from env_base import BaseEnvironment\n",
    "from env_base_test import BaseTestEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and environment initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_filename = \"001\"\n",
    "mat_filename = vid_filename + \".mat\"\n",
    "target_subject = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_bounding_boxes_per_frame, patch_centres_per_frame, speaker_info_per_frame = compute_frame_features(\n",
    "    vid_filename\n",
    ")\n",
    "\n",
    "foa_centres_per_frame_per_subject, patch_weights_per_frame = compute_foa_features(\n",
    "    mat_filename, patch_centres_per_frame\n",
    ")\n",
    "foa_centres_per_frame = [frame[target_subject] for frame in foa_centres_per_frame_per_subject]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_env = BaseEnvironment(\n",
    "    1,\n",
    "    patch_bounding_boxes_per_frame,\n",
    "    patch_centres_per_frame,\n",
    "    speaker_info_per_frame,\n",
    "    foa_centres_per_frame,\n",
    "    patch_weights_per_frame,\n",
    "    frame_width=320, # from data_utils.py\n",
    "    frame_height=180,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.observation_space.sample(), env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For efficiency, it's a good idea to set up some vectorized environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_envs = 5\n",
    "num_test_envs = 10\n",
    "\n",
    "train_envs = ts.env.DummyVectorEnv([lambda: markov_env for _ in range(num_train_envs)])\n",
    "test_envs = ts.env.DummyVectorEnv([lambda: markov_env for _ in range(num_test_envs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's construct the network.\n",
    "\n",
    "The biggest headache comes from the observations: they're quite complex. So, we build multiple networks, each processing a part of an observation and combining their outputs in the end!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, observation_space, action_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_patches = observation_space['patch_centres'].shape[0]\n",
    "\n",
    "        # network for patch_centres\n",
    "        self.patch_centres_net = nn.Sequential(\n",
    "            nn.Linear(np.prod(observation_space['patch_centres'].shape), 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # network for patch_bounding_boxes\n",
    "        self.patch_bboxes_net = nn.Sequential(\n",
    "            nn.Linear(np.prod(observation_space['patch_bounding_boxes'].shape), 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # network for speaker_info\n",
    "        self.speaker_info_net = nn.Sequential(\n",
    "            nn.Linear(np.prod(observation_space['speaker_info'].shape), 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # combining the outputs of all networks\n",
    "        self.combined_net = nn.Sequential(\n",
    "            nn.Linear(64 + 64 + 32, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, np.prod(action_shape))\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, state=None, info={}):\n",
    "        patch_centres = torch.tensor(obs['patch_centres'], dtype=torch.float32)\n",
    "        patch_bboxes = torch.tensor(obs['patch_bounding_boxes'], dtype=torch.float32)\n",
    "        speaker_info = torch.tensor(obs['speaker_info'], dtype=torch.float32)\n",
    "\n",
    "        patch_centres = patch_centres.view(patch_centres.size(0), -1)\n",
    "        patch_bboxes = patch_bboxes.view(patch_bboxes.size(0), -1)\n",
    "        speaker_info = speaker_info.view(speaker_info.size(0), -1)\n",
    "\n",
    "        # pass through respective networks\n",
    "        patch_centres_out = self.patch_centres_net(patch_centres)\n",
    "        patch_bboxes_out = self.patch_bboxes_net(patch_bboxes)\n",
    "        speaker_info_out = self.speaker_info_net(speaker_info)\n",
    "\n",
    "        # combine outputs\n",
    "        combined = torch.cat([patch_centres_out, patch_bboxes_out, speaker_info_out], dim=1)\n",
    "\n",
    "        logits = self.combined_net(combined)\n",
    "\n",
    "        return logits, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_shape = markov_env.observation_space\n",
    "action_shape = markov_env.action_space.n\n",
    "\n",
    "net = Net(state_shape, action_shape)\n",
    "optim = torch.optim.Adam(net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (patch_centres_net): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (patch_bboxes_net): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=64, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (speaker_info_net): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=32, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (combined_net): Sequential(\n",
       "    (0): Linear(in_features=160, out_features=128, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to set up the policy, which is readily done in Tianshou."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = ts.policy.DQNPolicy(\n",
    "    model=net, \n",
    "    optim=optim, \n",
    "    discount_factor=0.99,\n",
    "    estimation_step=1,\n",
    "    target_update_freq=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to set up the collectors, i.e., the objects that will be interacting with the environment according to the above policy and collect the generated data.\n",
    "\n",
    "In classical DQN fashion, we store the data in a replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_collector = ts.data.Collector(policy, train_envs, ts.data.VectorReplayBuffer(6000, num_train_envs))\n",
    "\n",
    "test_collector = ts.data.Collector(policy, test_envs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "num_steps_per_epoch = 300\n",
    "step_per_collect = 10\n",
    "episode_per_test = 5\n",
    "batch_size = 30 # one second of data (videos are at 30FPS)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%d%m%Y-%H%M%S\")\n",
    "log_path = os.path.join(\"logs\", \"dqn\", \"base\", f\"video_{vid_filename}\", f\"subject_{target_subject}\", timestamp)\n",
    "writer = SummaryWriter(log_path)\n",
    "logger = TensorboardLogger(writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 301it [00:00, 454.24it/s, env_step=300, len=0, loss=0.757, n/ep=0, n/st=10, rew=0.00]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: 413.805613 ± 344.469630, best_reward: 413.805613 ± 344.469630 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 301it [00:00, 501.56it/s, env_step=600, len=0, loss=0.954, n/ep=0, n/st=10, rew=0.00]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: 413.805613 ± 340.183569, best_reward: 413.805613 ± 344.469630 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 301it [00:00, 495.12it/s, env_step=900, len=0, loss=1.762, n/ep=0, n/st=10, rew=0.00]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3: test_reward: 413.805613 ± 332.180843, best_reward: 413.805613 ± 344.469630 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 301it [00:00, 490.03it/s, env_step=1200, len=0, loss=1.021, n/ep=0, n/st=10, rew=0.00]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4: test_reward: 413.805613 ± 342.603070, best_reward: 413.805613 ± 344.469630 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 301it [00:00, 494.13it/s, env_step=1500, len=0, loss=0.987, n/ep=0, n/st=10, rew=0.00]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5: test_reward: 413.805613 ± 335.459773, best_reward: 413.805613 ± 344.469630 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6: 301it [00:00, 493.19it/s, env_step=1800, len=0, loss=1.434, n/ep=0, n/st=10, rew=0.00]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #6: test_reward: 413.805613 ± 345.256310, best_reward: 413.805613 ± 344.469630 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #7: 301it [00:00, 496.84it/s, env_step=2100, len=0, loss=1.040, n/ep=0, n/st=10, rew=0.00]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #7: test_reward: 153.505796 ± 128.277340, best_reward: 413.805613 ± 344.469630 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #8: 301it [00:00, 491.92it/s, env_step=2400, len=0, loss=1.182, n/ep=0, n/st=10, rew=0.00]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #8: test_reward: 413.805613 ± 349.556102, best_reward: 413.805613 ± 344.469630 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #9: 301it [00:00, 486.89it/s, env_step=2700, len=0, loss=1.206, n/ep=0, n/st=10, rew=0.00]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #9: test_reward: 413.805613 ± 333.741414, best_reward: 413.805613 ± 344.469630 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #10: 301it [00:00, 497.67it/s, env_step=3000, len=0, loss=1.168, n/ep=0, n/st=10, rew=0.00]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #10: test_reward: 413.805613 ± 337.912158, best_reward: 413.805613 ± 344.469630 in #1\n"
     ]
    }
   ],
   "source": [
    "result = ts.trainer.offpolicy_trainer(\n",
    "    policy, \n",
    "    train_collector, \n",
    "    test_collector,\n",
    "    max_epoch=num_epochs,\n",
    "    step_per_epoch=num_steps_per_epoch,\n",
    "    step_per_collect=step_per_collect,\n",
    "    episode_per_test=episode_per_test,\n",
    "    batch_size=batch_size,\n",
    "    logger=logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the weights of the current module (for further testing and improvements down the line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_path = os.path.join(\"weights\", \"dqn\", \"base\", f\"video_{vid_filename}\", f\"subject_{target_subject}\")\n",
    "# TODO handle the error in case the path already exists (i.e., just continue)\n",
    "os.makedirs(policy_path)\n",
    "\n",
    "torch.save(policy.state_dict(), os.path.join(policy_path, \"weights.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'duration': '8.81s',\n",
       " 'train_time/model': '5.79s',\n",
       " 'test_step': 19789,\n",
       " 'test_episode': 55,\n",
       " 'test_time': '2.61s',\n",
       " 'test_speed': '7576.93 step/s',\n",
       " 'best_reward': 413.8056129167984,\n",
       " 'best_result': '413.81 ± 344.47',\n",
       " 'train_step': 3000,\n",
       " 'train_episode': 0,\n",
       " 'train_time/collector': '0.40s',\n",
       " 'train_speed': '484.14 step/s'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that's quite a let down...\n",
    "\n",
    "Although, there's not much to be surprised about: there is so very little information passed to the networks! \n",
    "\n",
    "Plus, I'm still not too sure that the problem is really amenable to RL in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, I have two choices:\n",
    "1. I keep fine-tuning hyperparameters until I get an acceptable result,\n",
    "2. I try a different approach.\n",
    "\n",
    "I'll opt for the second option, but I made this code into a notebook precisely because, that way, tinkering would be easier. So, if you wish to tune and fine-tune things, go ahead!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #! make sure you run all the code up to the instantiation of the models and optimizers before this cell\n",
    "policy = ts.policy.DQNPolicy(\n",
    "    model=net, \n",
    "    optim=optim, \n",
    "    discount_factor=0.99,\n",
    "    estimation_step=1,\n",
    "    target_update_freq=50\n",
    ")\n",
    "\n",
    "policy.load_state_dict(torch.load(os.path.join(policy_path, \"weights.pth\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_markov_env = BaseTestEnvironment(\n",
    "    1,\n",
    "    patch_bounding_boxes_per_frame,\n",
    "    patch_centres_per_frame,\n",
    "    speaker_info_per_frame,\n",
    "    foa_centres_per_frame,\n",
    "    patch_weights_per_frame,\n",
    "    frame_width=320, # from data_utils.py\n",
    "    frame_height=180,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_envs = 1 # need set it to 1 (else it doesn't get to the end)\n",
    "\n",
    "testing_envs = ts.env.DummyVectorEnv([lambda: test_markov_env for _ in range(num_test_envs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n/ep': 3, 'n/st': 1797, 'rews': array([400., 400., 400.]), 'lens': array([599, 599, 599]), 'idxs': array([0, 0, 0]), 'rew': 400.0, 'len': 599.0, 'rew_std': 0.0, 'len_std': 0.0}\n"
     ]
    }
   ],
   "source": [
    "policy.eval()\n",
    "policy.set_eps(0.05)\n",
    "\n",
    "collector = ts.data.Collector(policy, testing_envs)\n",
    "# should be the same values 3 times (if not, there's a problem)\n",
    "print(collector.collect(n_episode=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 3699), started 0:00:20 ago. (Use '!kill 3699' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-fb380239f86db6b5\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-fb380239f86db6b5\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/dqn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
