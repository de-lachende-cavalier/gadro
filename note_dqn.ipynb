{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A first stab: DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[DQN](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) is a classical RL algorithm which should provide a nice baseline for further work.\n",
    "\n",
    "Classical RL techniques woul probably not work very well without further feature engineering, because the current state space is quite large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tianshou as ts \n",
    "from tianshou.utils import TensorboardLogger\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johndoe/Desktop/uni/affectivecompute/project/lib/python3.11/site-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "from utils_preprocess import compute_frame_features, compute_foa_features\n",
    "\n",
    "from env_markov import MarkovGazeEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and environment initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_filename = \"001\"\n",
    "mat_filename = vid_filename + \".mat\"\n",
    "target_subject = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_bounding_boxes_per_frame, patch_centres_per_frame, speaker_info_per_frame = compute_frame_features(\n",
    "    vid_filename\n",
    ")\n",
    "\n",
    "foa_centres_per_frame_per_subject, patch_weights_per_frame = compute_foa_features(\n",
    "    mat_filename, patch_centres_per_frame\n",
    ")\n",
    "foa_centres_per_frame = [frame[target_subject] for frame in foa_centres_per_frame_per_subject]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MarkovGazeEnv(\n",
    "    patch_bounding_boxes_per_frame,\n",
    "    patch_centres_per_frame,\n",
    "    speaker_info_per_frame,\n",
    "    foa_centres_per_frame,\n",
    "    patch_weights_per_frame,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.observation_space.sample(), env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For efficiency, it's a good idea to set up some vectorized environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_envs = 5\n",
    "num_test_envs = 10\n",
    "\n",
    "train_envs = ts.env.DummyVectorEnv([lambda: env for _ in range(num_train_envs)])\n",
    "test_envs = ts.env.DummyVectorEnv([lambda: env for _ in range(num_test_envs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's construct the network.\n",
    "\n",
    "The biggest headache comes from the observations: they're quite complex. So, we build multiple networks, each processing a part of an observation and combining their outputs in the end!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, observation_space, action_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_patches = observation_space['patch_centres'].shape[0]\n",
    "\n",
    "        # network for patch_centres\n",
    "        self.patch_centres_net = nn.Sequential(\n",
    "            nn.Linear(np.prod(observation_space['patch_centres'].shape), 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # network for patch_bounding_boxes\n",
    "        self.patch_bboxes_net = nn.Sequential(\n",
    "            nn.Linear(np.prod(observation_space['patch_bounding_boxes'].shape), 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # network for speaker_info\n",
    "        self.speaker_info_net = nn.Sequential(\n",
    "            nn.Linear(np.prod(observation_space['speaker_info'].shape), 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # combining the outputs of all networks\n",
    "        self.combined_net = nn.Sequential(\n",
    "            nn.Linear(64 + 64 + 32, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, np.prod(action_shape))\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, state=None, info={}):\n",
    "        patch_centres = torch.tensor(obs['patch_centres'], dtype=torch.float32)\n",
    "        patch_bboxes = torch.tensor(obs['patch_bounding_boxes'], dtype=torch.float32)\n",
    "        speaker_info = torch.tensor(obs['speaker_info'], dtype=torch.float32)\n",
    "\n",
    "        patch_centres = patch_centres.view(patch_centres.size(0), -1)\n",
    "        patch_bboxes = patch_bboxes.view(patch_bboxes.size(0), -1)\n",
    "        speaker_info = speaker_info.view(speaker_info.size(0), -1)\n",
    "\n",
    "        # pass through respective networks\n",
    "        patch_centres_out = self.patch_centres_net(patch_centres)\n",
    "        patch_bboxes_out = self.patch_bboxes_net(patch_bboxes)\n",
    "        speaker_info_out = self.speaker_info_net(speaker_info)\n",
    "\n",
    "        # combine outputs\n",
    "        combined = torch.cat([patch_centres_out, patch_bboxes_out, speaker_info_out], dim=1)\n",
    "\n",
    "        logits = self.combined_net(combined)\n",
    "\n",
    "        return logits, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_shape = env.observation_space\n",
    "action_shape = env.action_space.n\n",
    "\n",
    "net = Net(state_shape, action_shape)\n",
    "optim = torch.optim.Adam(net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (patch_centres_net): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (patch_bboxes_net): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=64, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (speaker_info_net): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=32, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (combined_net): Sequential(\n",
       "    (0): Linear(in_features=160, out_features=128, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to set up the policy, which is readily done in Tianshou."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = ts.policy.DQNPolicy(\n",
    "    model=net, \n",
    "    optim=optim, \n",
    "    discount_factor=0.99,\n",
    "    estimation_step=1,\n",
    "    target_update_freq=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to set up the collectors, i.e., the objects that will be interacting with the environment according to the above policy and collect the generated data.\n",
    "\n",
    "In classical DQN fashion, we store the data in a replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_collector = ts.data.Collector(policy, train_envs, ts.data.VectorReplayBuffer(6000, num_train_envs))\n",
    "\n",
    "test_collector = ts.data.Collector(policy, test_envs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "num_steps_per_epoch = 3000\n",
    "step_per_collect = 10 # update the Q-values at each step\n",
    "episode_per_test = 5 # use 5 episodes to test the policy\n",
    "batch_size = 30 # one second of data (videos are at 30FPS)\n",
    "\n",
    "log_path = os.path.join(\"logs\", \"dqn\")\n",
    "writer = SummaryWriter(log_path)\n",
    "logger = TensorboardLogger(writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 3001it [00:08, 368.63it/s, env_step=3000, len=120, loss=6.392, n/ep=2, n/st=10, rew=77.29]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: 230.130769 ± 181.954642, best_reward: 230.130769 ± 206.245826 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 3001it [00:08, 368.25it/s, env_step=6000, len=120, loss=8.135, n/ep=2, n/st=10, rew=73.72]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: 230.130769 ± 186.879228, best_reward: 230.130769 ± 206.245826 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 3001it [00:08, 370.53it/s, env_step=9000, len=120, loss=11.961, n/ep=2, n/st=10, rew=56.41]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3: test_reward: 230.130769 ± 185.320019, best_reward: 230.130769 ± 206.245826 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 3001it [00:07, 375.70it/s, env_step=12000, len=120, loss=8.714, n/ep=2, n/st=10, rew=83.98]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4: test_reward: 230.130769 ± 165.187940, best_reward: 230.130769 ± 206.245826 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 3001it [00:08, 372.83it/s, env_step=15000, len=120, loss=8.472, n/ep=2, n/st=10, rew=75.37]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5: test_reward: 230.130769 ± 180.315986, best_reward: 230.130769 ± 206.245826 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6: 3001it [00:08, 370.10it/s, env_step=18000, len=120, loss=13.260, n/ep=2, n/st=10, rew=80.25]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #6: test_reward: 230.130769 ± 181.332813, best_reward: 230.130769 ± 206.245826 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #7: 3001it [00:08, 367.54it/s, env_step=21000, len=120, loss=12.444, n/ep=2, n/st=10, rew=72.66]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #7: test_reward: 230.130769 ± 171.971820, best_reward: 230.130769 ± 206.245826 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #8: 3001it [00:08, 370.68it/s, env_step=24000, len=120, loss=8.162, n/ep=2, n/st=10, rew=73.53]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #8: test_reward: 230.130769 ± 182.899976, best_reward: 230.130769 ± 206.245826 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #9: 3001it [00:08, 365.29it/s, env_step=27000, len=120, loss=9.022, n/ep=2, n/st=10, rew=87.14]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #9: test_reward: 230.130769 ± 193.127519, best_reward: 230.130769 ± 206.245826 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #10: 3001it [00:08, 364.94it/s, env_step=30000, len=120, loss=10.418, n/ep=2, n/st=10, rew=76.90]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #10: test_reward: 230.130769 ± 172.559083, best_reward: 230.130769 ± 206.245826 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #11: 3001it [00:08, 370.88it/s, env_step=33000, len=120, loss=16.843, n/ep=2, n/st=10, rew=67.07]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #11: test_reward: 230.130769 ± 202.285166, best_reward: 230.130769 ± 206.245826 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #12: 3001it [00:08, 368.15it/s, env_step=36000, len=120, loss=9.707, n/ep=2, n/st=10, rew=69.39]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #12: test_reward: 230.130769 ± 196.513353, best_reward: 230.130769 ± 206.245826 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #13: 3001it [00:08, 355.61it/s, env_step=39000, len=120, loss=9.075, n/ep=2, n/st=10, rew=74.49]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #13: test_reward: 230.130769 ± 171.016749, best_reward: 230.130769 ± 206.245826 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #14: 3001it [00:08, 365.86it/s, env_step=42000, len=120, loss=9.821, n/ep=2, n/st=10, rew=74.63]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #14: test_reward: 230.130769 ± 171.700521, best_reward: 230.130769 ± 206.245826 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #15: 3001it [00:08, 362.16it/s, env_step=45000, len=120, loss=12.389, n/ep=2, n/st=10, rew=70.22]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #15: test_reward: 230.130769 ± 176.434622, best_reward: 230.130769 ± 206.245826 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #16: 3001it [00:08, 369.72it/s, env_step=48000, len=120, loss=6.614, n/ep=2, n/st=10, rew=76.40]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #16: test_reward: 230.130769 ± 174.786172, best_reward: 230.130769 ± 206.245826 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #17: 3001it [00:08, 373.51it/s, env_step=51000, len=120, loss=10.995, n/ep=2, n/st=10, rew=75.51]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #17: test_reward: 230.130769 ± 180.954456, best_reward: 230.130769 ± 206.245826 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #18: 3001it [00:08, 360.65it/s, env_step=54000, len=120, loss=9.811, n/ep=2, n/st=10, rew=77.64]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #18: test_reward: 230.130769 ± 192.108256, best_reward: 230.130769 ± 206.245826 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #19: 3001it [00:08, 368.97it/s, env_step=57000, len=120, loss=13.133, n/ep=2, n/st=10, rew=65.03]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #19: test_reward: 230.130769 ± 170.978114, best_reward: 230.130769 ± 206.245826 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #20: 3001it [00:08, 363.16it/s, env_step=60000, len=120, loss=18.336, n/ep=2, n/st=10, rew=76.37]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #20: test_reward: 230.130769 ± 189.504165, best_reward: 230.130769 ± 206.245826 in #0\n"
     ]
    }
   ],
   "source": [
    "result = ts.trainer.offpolicy_trainer(\n",
    "    policy, \n",
    "    train_collector, \n",
    "    test_collector,\n",
    "    max_epoch=num_epochs,\n",
    "    step_per_epoch=num_steps_per_epoch,\n",
    "    step_per_collect=step_per_collect,\n",
    "    episode_per_test=episode_per_test,\n",
    "    batch_size=batch_size,\n",
    "    logger=logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'duration': '168.85s',\n",
       " 'train_time/model': '154.27s',\n",
       " 'test_step': 37779,\n",
       " 'test_episode': 105,\n",
       " 'test_time': '5.49s',\n",
       " 'test_speed': '6882.52 step/s',\n",
       " 'best_reward': 230.13076861072403,\n",
       " 'best_result': '230.13 ± 206.25',\n",
       " 'train_step': 60000,\n",
       " 'train_episode': 200,\n",
       " 'train_time/collector': '9.09s',\n",
       " 'train_speed': '367.28 step/s'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that's quite a let down...\n",
    "\n",
    "Although, there's not much to be surprised about: there is so very little information passed to the networks! \n",
    "\n",
    "Plus, I'm still not too sure that the problem is really amenable to RL in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, I have two choices:\n",
    "1. I keep fine-tuning hyperparameters until I get an acceptable result,\n",
    "2. I try a different approach.\n",
    "\n",
    "I'll opt for the second option, but I made this code into a notebook precisely because, that way, tinkering would be easier. So, if you wish to tune and fine-tune things, go ahead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n/ep': 10,\n",
       " 'n/st': 3000,\n",
       " 'rews': array([ 91.27054762,  77.12606428,  67.49008293,  77.55667718,\n",
       "         95.07588698,  56.58696209, 329.79185276,  88.81977665,\n",
       "        480.1971609 , 551.34139371]),\n",
       " 'lens': array([120, 120, 120, 120, 120, 120, 510, 150, 810, 810]),\n",
       " 'idxs': array([3, 4, 3, 4, 3, 4, 2, 4, 0, 1]),\n",
       " 'rew': 191.5256405089367,\n",
       " 'len': 300.0,\n",
       " 'rew_std': 179.30079631351055,\n",
       " 'len_std': 279.4995527724508}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.eval()\n",
    "policy.set_eps(0.05)\n",
    "\n",
    "collector = ts.data.Collector(policy, train_envs)\n",
    "collector.collect(n_episode=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does perform quite well, every once in a while..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 5009), started 0:02:14 ago. (Use '!kill 5009' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b11c1e5f9fd8fc17\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b11c1e5f9fd8fc17\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/dqn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
