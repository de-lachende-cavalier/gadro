{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imitation learning with GAIL (Generative Adversarial Imitation Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tianshou as ts \n",
    "from tianshou.utils import TensorboardLogger\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johndoe/Desktop/uni/affectivecompute/project/lib/python3.11/site-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "from utils_preprocess import compute_frame_features, compute_foa_features\n",
    "from utils_data import get_frames_by_type, get_feature_frames\n",
    "\n",
    "from env_frames import FramesEnvironment\n",
    "from env_frames_test import FramesTestEnvironment\n",
    "\n",
    "from policy_gail_prime import GAILPrimePolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_filename = \"012\"\n",
    "mat_filename = vid_filename + \".mat\"\n",
    "target_subject = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_frames = get_frames_by_type(\"frames\", vid_filename)\n",
    "dynamics_frames = get_frames_by_type(\"dynamic\", vid_filename)\n",
    "patches_frames, _ = get_feature_frames(vid_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to normalise frames before feeding them into the model\n",
    "for i in range(len(video_frames)):\n",
    "    # broadcasting will do the rest...\n",
    "    video_frames[i] = video_frames[i] / 255.0\n",
    "    dynamics_frames[i] = dynamics_frames[i] / 255.0\n",
    "    patches_frames[i] = patches_frames[i] / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_bounding_boxes_per_frame, patch_centres_per_frame, speaker_info_per_frame = compute_frame_features(\n",
    "    vid_filename\n",
    ")\n",
    "\n",
    "foa_centres_per_frame_per_subject, patch_weights_per_frame = compute_foa_features(\n",
    "    mat_filename, patch_centres_per_frame\n",
    ")\n",
    "foa_centres_per_frame = [frame[target_subject] for frame in foa_centres_per_frame_per_subject]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_env = FramesEnvironment(\n",
    "    1,\n",
    "    video_frames,\n",
    "    dynamics_frames,\n",
    "    patches_frames,\n",
    "    patch_bounding_boxes_per_frame,\n",
    "    patch_centres_per_frame,\n",
    "    speaker_info_per_frame,\n",
    "    foa_centres_per_frame,\n",
    "    patch_weights_per_frame,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_envs = 5\n",
    "num_test_envs = 3\n",
    "\n",
    "train_envs = ts.env.DummyVectorEnv([lambda: markov_env for _ in range(num_train_envs)])\n",
    "test_envs = ts.env.DummyVectorEnv([lambda: markov_env for _ in range(num_test_envs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNet(nn.Module):\n",
    "    def __init__(self, observation_space, action_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_patches = observation_space['patch_centres'].shape[0]\n",
    "        self.output_dim = np.prod(action_shape)\n",
    "\n",
    "        self.rgb_frame_net = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=5, stride=2, padding=2),  # [3, 180, 320]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # [16, 45, 80]\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=2, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # [32, 11, 20]\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),  # [64, 6, 10]\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 6 * 10, 64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.bitmap_frame_net = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=2, padding=2),  # [1, 180, 320]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # [16, 45, 80]\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=2, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # [32, 11, 20]\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),  # [64, 6, 10]\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 6 * 10, 64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # network for patch_centres\n",
    "        self.patch_centres_net = nn.Sequential(\n",
    "            nn.Linear(np.prod(observation_space['patch_centres'].shape), 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # network for speaker_info\n",
    "        self.speaker_info_net = nn.Sequential(\n",
    "            nn.Linear(np.prod(observation_space['speaker_info'].shape), 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # combining the outputs of all networks\n",
    "        self.combined_net = nn.Sequential(\n",
    "            nn.Linear(64 + 32 + 64 + 64 + 64, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, np.prod(action_shape), bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, state=None, info={}):\n",
    "        patch_centres = torch.tensor(obs['patch_centres'], dtype=torch.float32)\n",
    "        speaker_info = torch.tensor(obs['speaker_info'], dtype=torch.float32)\n",
    "        \n",
    "        patch_centres = patch_centres.view(patch_centres.size(0), -1)\n",
    "        speaker_info = speaker_info.view(speaker_info.size(0), -1)\n",
    "\n",
    "        video_frame = torch.tensor(obs['video_frames'], dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "        dynamics_frame = torch.tensor(obs['dynamics_frames'], dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "        patches_frame = torch.tensor(obs['patches_frames'], dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "        # pass through respective networks\n",
    "        patch_centres_out = self.patch_centres_net(patch_centres)\n",
    "        speaker_info_out = self.speaker_info_net(speaker_info)\n",
    "\n",
    "        video_frame_out = self.rgb_frame_net(video_frame)\n",
    "        dynamics_frame_out = self.rgb_frame_net(dynamics_frame)\n",
    "        patches_frame_out = self.bitmap_frame_net(patches_frame)\n",
    "\n",
    "        # combine outputs\n",
    "        combined = torch.cat([patch_centres_out, speaker_info_out, video_frame_out, dynamics_frame_out, patches_frame_out], dim=1)\n",
    "\n",
    "        logits = self.combined_net(combined)\n",
    "\n",
    "        return logits, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNet(nn.Module):\n",
    "    def __init__(self, observation_space):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rgb_frame_net = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=5, stride=2, padding=2),  # [3, 180, 320]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # [16, 45, 80]\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=2, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # [32, 11, 20]\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),  # [64, 6, 10]\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 6 * 10, 64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.bitmap_frame_net = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=2, padding=2),  # [1, 180, 320]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # [16, 45, 80]\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=2, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # [32, 11, 20]\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),  # [64, 6, 10]\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 6 * 10, 64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # network for patch_centres\n",
    "        self.patch_centres_net = nn.Sequential(\n",
    "            nn.Linear(np.prod(observation_space['patch_centres'].shape), 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # network for speaker_info\n",
    "        self.speaker_info_net = nn.Sequential(\n",
    "            nn.Linear(np.prod(observation_space['speaker_info'].shape), 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # combining the outputs of all networks\n",
    "        self.combined_net = nn.Sequential(\n",
    "            nn.Linear(64 + 32 + 64 + 64 + 64, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 1, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, state=None, info={}):\n",
    "        patch_centres = torch.tensor(obs['patch_centres'], dtype=torch.float32)\n",
    "        speaker_info = torch.tensor(obs['speaker_info'], dtype=torch.float32)\n",
    "        \n",
    "        patch_centres = patch_centres.view(patch_centres.size(0), -1)\n",
    "        speaker_info = speaker_info.view(speaker_info.size(0), -1)\n",
    "\n",
    "        video_frame = torch.tensor(obs['video_frames'], dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "        dynamics_frame = torch.tensor(obs['dynamics_frames'], dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "        patches_frame = torch.tensor(obs['patches_frames'], dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "        patch_centres_out = self.patch_centres_net(patch_centres)\n",
    "        speaker_info_out = self.speaker_info_net(speaker_info)\n",
    "\n",
    "        video_frame_out = self.rgb_frame_net(video_frame)\n",
    "        dynamics_frame_out = self.rgb_frame_net(dynamics_frame)\n",
    "        patches_frame_out = self.bitmap_frame_net(patches_frame)\n",
    "\n",
    "        combined = torch.cat([patch_centres_out, speaker_info_out, video_frame_out, dynamics_frame_out, patches_frame_out], dim=1)\n",
    "        state_value = self.combined_net(combined)\n",
    "\n",
    "        return state_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need a further network for GAIL: the discriminator network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorNet(nn.Module):\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        super().__init__()\n",
    "\n",
    "        #! everything in this notebook runs on CPU, so if you want to change the device here, remember to do that for EVERY tensor and EVERY network\n",
    "        self.device = \"cpu\"\n",
    "\n",
    "        self.rgb_frame_net = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=5, stride=2, padding=2),  # [3, 180, 320]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # [16, 45, 80]\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=2, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # [32, 11, 20]\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),  # [64, 6, 10]\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 6 * 10, 64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.bitmap_frame_net = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=2, padding=2),  # [1, 180, 320]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # [16, 45, 80]\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=2, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # [32, 11, 20]\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),  # [64, 6, 10]\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 6 * 10, 64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # network for patch_centres\n",
    "        self.patch_centres_net = nn.Sequential(\n",
    "            nn.Linear(np.prod(observation_space['patch_centres'].shape), 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # network for speaker_info\n",
    "        self.speaker_info_net = nn.Sequential(\n",
    "            nn.Linear(np.prod(observation_space['speaker_info'].shape), 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # combining the outputs of all networks\n",
    "        self.action_net = nn.Sequential(\n",
    "            nn.Linear(np.prod(action_space), 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "\n",
    "        # combining the outputs of all networks\n",
    "        self.combined_net = nn.Sequential(\n",
    "            nn.Linear(64 + 32 + 64 + 64 + 64 + 16, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 1, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, oh_action, state=None, info={}):\n",
    "        patch_centres = torch.tensor(obs['patch_centres'], dtype=torch.float32)\n",
    "        speaker_info = torch.tensor(obs['speaker_info'], dtype=torch.float32)\n",
    "        \n",
    "        patch_centres = patch_centres.view(patch_centres.size(0), -1)\n",
    "        speaker_info = speaker_info.view(speaker_info.size(0), -1)\n",
    "\n",
    "        video_frame = torch.tensor(obs['video_frames'], dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "        dynamics_frame = torch.tensor(obs['dynamics_frames'], dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "        patches_frame = torch.tensor(obs['patches_frames'], dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "        patch_centres_out = self.patch_centres_net(patch_centres)\n",
    "        speaker_info_out = self.speaker_info_net(speaker_info)\n",
    "\n",
    "        video_frame_out = self.rgb_frame_net(video_frame)\n",
    "        dynamics_frame_out = self.rgb_frame_net(dynamics_frame)\n",
    "        patches_frame_out = self.bitmap_frame_net(patches_frame)\n",
    "\n",
    "        action_out = self.action_net(oh_action)\n",
    "\n",
    "        combined = torch.cat([patch_centres_out, speaker_info_out, video_frame_out, dynamics_frame_out, patches_frame_out, action_out], dim=1)\n",
    "        combined_out = self.combined_net(combined)\n",
    "\n",
    "        return combined_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_shape = markov_env.observation_space\n",
    "action_shape = markov_env.action_space.n\n",
    "\n",
    "actor_net = ActorNet(state_shape, action_shape)\n",
    "critic_net = CriticNet(state_shape)\n",
    "discriminator_net = DiscriminatorNet(state_shape, action_shape)\n",
    "\n",
    "# using a single optimizer for actor and critic simplifies the training loop and is more computationally efficient (plus it guarantees some consistency across networks)\n",
    "# BUT gradient updates in one network will influence the gradient updates in the other, and this might create unexpected problems...\n",
    "combined_params = list(actor_net.parameters()) + list(critic_net.parameters())\n",
    "optimizer = torch.optim.Adam(combined_params, lr=3e-4)\n",
    "\n",
    "disc_params = list(discriminator_net.parameters())\n",
    "disc_optimizer = torch.optim.Adam(disc_params, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorNet(\n",
       "  (rgb_frame_net): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(16, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Flatten(start_dim=1, end_dim=-1)\n",
       "    (9): Linear(in_features=3840, out_features=64, bias=True)\n",
       "    (10): ReLU(inplace=True)\n",
       "  )\n",
       "  (bitmap_frame_net): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(16, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Flatten(start_dim=1, end_dim=-1)\n",
       "    (9): Linear(in_features=3840, out_features=64, bias=True)\n",
       "    (10): ReLU(inplace=True)\n",
       "  )\n",
       "  (patch_centres_net): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=64, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (speaker_info_net): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (combined_net): Sequential(\n",
       "    (0): Linear(in_features=288, out_features=128, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=128, out_features=4, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CriticNet(\n",
       "  (rgb_frame_net): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(16, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Flatten(start_dim=1, end_dim=-1)\n",
       "    (9): Linear(in_features=3840, out_features=64, bias=True)\n",
       "    (10): ReLU(inplace=True)\n",
       "  )\n",
       "  (bitmap_frame_net): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(16, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Flatten(start_dim=1, end_dim=-1)\n",
       "    (9): Linear(in_features=3840, out_features=64, bias=True)\n",
       "    (10): ReLU(inplace=True)\n",
       "  )\n",
       "  (patch_centres_net): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=64, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (speaker_info_net): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (combined_net): Sequential(\n",
       "    (0): Linear(in_features=288, out_features=128, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=128, out_features=1, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critic_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiscriminatorNet(\n",
       "  (rgb_frame_net): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(16, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Flatten(start_dim=1, end_dim=-1)\n",
       "    (9): Linear(in_features=3840, out_features=64, bias=True)\n",
       "    (10): ReLU(inplace=True)\n",
       "  )\n",
       "  (bitmap_frame_net): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(16, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Flatten(start_dim=1, end_dim=-1)\n",
       "    (9): Linear(in_features=3840, out_features=64, bias=True)\n",
       "    (10): ReLU(inplace=True)\n",
       "  )\n",
       "  (patch_centres_net): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=64, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (speaker_info_net): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (action_net): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "  )\n",
       "  (combined_net): Sequential(\n",
       "    (0): Linear(in_features=304, out_features=128, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=128, out_features=1, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up GAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expert_policy(info):\n",
    "    next_frame_idx = info[\"current_frame_idx\"] + 1\n",
    "    attended_centre = foa_centres_per_frame[next_frame_idx]\n",
    "\n",
    "    # always picks the correct patch\n",
    "    return np.array(patch_centres_per_frame[next_frame_idx].index(attended_centre))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need a buffer of expert data, which is meant to be imitated by our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 1000 \n",
    "expert_buffer = ts.data.ReplayBuffer(size=buffer_size)\n",
    "\n",
    "obs, info = markov_env.reset()\n",
    "for _ in range(buffer_size):\n",
    "    action = expert_policy(info) \n",
    "\n",
    "    next_obs, _, terminated, truncated, next_info = markov_env.step(action)\n",
    "    \n",
    "    # the reward is not used by GAIL, but is necessary, as per Tianshou API\n",
    "    expert_buffer.add(ts.data.Batch(obs=obs, act=action, rew=0.0, obs_next=next_obs, terminated=terminated, truncated=truncated))\n",
    "    \n",
    "    if terminated:\n",
    "        # reset the environment if we terminate before filling the buffer\n",
    "        obs, info = markov_env.reset()\n",
    "    else:\n",
    "        obs, info = next_obs, next_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_fn(logits: torch.Tensor):\n",
    "    return torch.distributions.Categorical(logits=logits)\n",
    "\n",
    "policy = GAILPrimePolicy(\n",
    "    actor_net, \n",
    "    critic_net, \n",
    "    optimizer,\n",
    "    dist_fn,\n",
    "    expert_buffer,\n",
    "    discriminator_net,\n",
    "    disc_optimizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_collector = ts.data.Collector(policy, train_envs)\n",
    "\n",
    "test_collector = ts.data.Collector(policy, test_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "num_steps_per_epoch = 500\n",
    "step_per_collect = 5\n",
    "episode_per_test = 3\n",
    "batch_size = 10\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%d%m%Y-%H%M%S\")\n",
    "log_path = os.path.join(\"logs\", \"gail\", \"frames\", f\"video_{vid_filename}\", f\"subject_{target_subject}\", timestamp)\n",
    "writer = SummaryWriter(log_path)\n",
    "logger = TensorboardLogger(writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 501it [00:35, 14.30it/s, env_step=500, len=100, loss=5.013, loss/clip=-0.000, loss/disc=1.364, loss/ent=1.252, loss/vf=10.052, n/ep=0, n/st=5, rew=29.07, stats/acc_exp=0.680, stats/acc_pi=0.470]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: 101.768463 ± 75.373435, best_reward: 117.034717 ± 83.713906 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 501it [00:33, 14.89it/s, env_step=1000, len=100, loss=4.777, loss/clip=0.000, loss/disc=1.292, loss/ent=1.138, loss/vf=9.577, n/ep=0, n/st=5, rew=33.22, stats/acc_exp=0.630, stats/acc_pi=0.690]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: 94.542585 ± 78.930204, best_reward: 117.034717 ± 83.713906 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 501it [00:33, 14.98it/s, env_step=1500, len=100, loss=4.465, loss/clip=0.000, loss/disc=1.351, loss/ent=1.067, loss/vf=8.952, n/ep=0, n/st=5, rew=44.56, stats/acc_exp=0.700, stats/acc_pi=0.440]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3: test_reward: 91.719925 ± 69.954405, best_reward: 117.034717 ± 83.713906 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 501it [00:33, 14.94it/s, env_step=2000, len=100, loss=4.105, loss/clip=0.000, loss/disc=1.332, loss/ent=1.219, loss/vf=8.235, n/ep=0, n/st=5, rew=29.38, stats/acc_exp=0.680, stats/acc_pi=0.510]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4: test_reward: 86.856749 ± 54.105594, best_reward: 117.034717 ± 83.713906 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 501it [00:34, 14.43it/s, env_step=2500, len=100, loss=4.399, loss/clip=0.000, loss/disc=1.312, loss/ent=1.130, loss/vf=8.821, n/ep=0, n/st=5, rew=35.90, stats/acc_exp=0.710, stats/acc_pi=0.500]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5: test_reward: 109.905463 ± 90.921338, best_reward: 117.034717 ± 83.713906 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6: 501it [00:38, 13.07it/s, env_step=3000, len=100, loss=4.051, loss/clip=0.000, loss/disc=1.306, loss/ent=1.103, loss/vf=8.124, n/ep=0, n/st=5, rew=35.64, stats/acc_exp=0.680, stats/acc_pi=0.610]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #6: test_reward: 114.317953 ± 95.193676, best_reward: 117.034717 ± 83.713906 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #7: 501it [00:34, 14.68it/s, env_step=3500, len=100, loss=3.970, loss/clip=0.000, loss/disc=1.332, loss/ent=0.978, loss/vf=7.960, n/ep=0, n/st=5, rew=35.34, stats/acc_exp=0.350, stats/acc_pi=0.720]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #7: test_reward: 113.634080 ± 100.634117, best_reward: 117.034717 ± 83.713906 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #8: 501it [00:35, 14.05it/s, env_step=4000, len=100, loss=3.823, loss/clip=0.000, loss/disc=1.375, loss/ent=0.874, loss/vf=7.664, n/ep=0, n/st=5, rew=43.80, stats/acc_exp=0.310, stats/acc_pi=0.760]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #8: test_reward: 127.086236 ± 111.766529, best_reward: 127.086236 ± 111.766529 in #8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #9: 501it [00:35, 14.29it/s, env_step=4500, len=100, loss=3.915, loss/clip=0.000, loss/disc=1.386, loss/ent=0.879, loss/vf=7.847, n/ep=0, n/st=5, rew=36.48, stats/acc_exp=0.390, stats/acc_pi=0.690]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #9: test_reward: 116.353462 ± 102.959098, best_reward: 127.086236 ± 111.766529 in #8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #10: 501it [00:33, 15.03it/s, env_step=5000, len=100, loss=4.067, loss/clip=-0.000, loss/disc=1.294, loss/ent=0.995, loss/vf=8.153, n/ep=0, n/st=5, rew=32.77, stats/acc_exp=0.730, stats/acc_pi=0.480]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #10: test_reward: 104.232144 ± 87.981237, best_reward: 127.086236 ± 111.766529 in #8\n"
     ]
    }
   ],
   "source": [
    "result = ts.trainer.onpolicy_trainer(\n",
    "    policy, \n",
    "    train_collector, \n",
    "    test_collector,\n",
    "    repeat_per_collect=1,\n",
    "    max_epoch=num_epochs,\n",
    "    step_per_epoch=num_steps_per_epoch,\n",
    "    step_per_collect=step_per_collect,\n",
    "    episode_per_test=episode_per_test,\n",
    "    batch_size=batch_size,\n",
    "    logger=logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'duration': '397.15s',\n",
       " 'train_time/model': '323.99s',\n",
       " 'test_step': 8701,\n",
       " 'test_episode': 33,\n",
       " 'test_time': '50.21s',\n",
       " 'test_speed': '173.30 step/s',\n",
       " 'best_reward': 127.08623618715016,\n",
       " 'best_result': '127.09 ± 111.77',\n",
       " 'train_step': 5000,\n",
       " 'train_episode': 10,\n",
       " 'train_time/collector': '22.96s',\n",
       " 'train_speed': '14.41 step/s'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n/ep': 10,\n",
       " 'n/st': 3162,\n",
       " 'rews': array([ 22.22135627,  28.51254907,  25.16103925,  27.08127978,\n",
       "         30.67060244,  23.82440383, 203.39745404, 218.22130575,\n",
       "        255.43788278, 283.68417918]),\n",
       " 'lens': array([ 79,  79,  79,  79,  79,  79, 573, 573, 771, 771]),\n",
       " 'idxs': array([4, 4, 4, 4, 4, 4, 2, 3, 0, 1]),\n",
       " 'rew': 111.82120523819722,\n",
       " 'len': 316.2,\n",
       " 'rew_std': 106.70425692970272,\n",
       " 'len_std': 297.1803492830574}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_markov_env = FramesTestEnvironment(\n",
    "    1,\n",
    "    video_frames,\n",
    "    dynamics_frames,\n",
    "    patches_frames,\n",
    "    patch_bounding_boxes_per_frame,\n",
    "    patch_centres_per_frame,\n",
    "    speaker_info_per_frame,\n",
    "    foa_centres_per_frame,\n",
    "    patch_weights_per_frame,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_envs = 1\n",
    "\n",
    "testing_envs = ts.env.DummyVectorEnv([lambda: test_markov_env for _ in range(num_test_envs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.eval()\n",
    "policy.set_eps(0.05)\n",
    "\n",
    "collector = ts.data.Collector(policy, testing_envs)\n",
    "# should be the same values 3 times (if not, there's a problem)\n",
    "collector.collect(n_episode=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
