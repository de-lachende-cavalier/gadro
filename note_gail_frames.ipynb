{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imitation learning with GAIL (Generative Adversarial Imitation Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tianshou as ts \n",
    "from tianshou.utils import TensorboardLogger\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_preprocess import compute_frame_features, compute_foa_features\n",
    "from utils_data import get_frames_by_type, get_feature_frames\n",
    "\n",
    "from env_frames_flat import FlatFramesEnvironment\n",
    "from gail_prime import GAILPrimePolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_filename = \"012\"\n",
    "mat_filename = vid_filename + \".mat\"\n",
    "target_subject = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_frames = get_frames_by_type(\"frames\", vid_filename)\n",
    "dynamics_frames = get_frames_by_type(\"dynamic\", vid_filename)\n",
    "patches_frames, _ = get_feature_frames(vid_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to normalise frames before feeding them into the model\n",
    "for i in range(len(video_frames)):\n",
    "    # broadcasting will do the rest...\n",
    "    video_frames[i] = video_frames[i] / 255.0\n",
    "    dynamics_frames[i] = dynamics_frames[i] / 255.0\n",
    "    patches_frames[i] = patches_frames[i] / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_bounding_boxes_per_frame, patch_centres_per_frame, speaker_info_per_frame = compute_frame_features(\n",
    "    vid_filename\n",
    ")\n",
    "\n",
    "foa_centres_per_frame_per_subject, patch_weights_per_frame = compute_foa_features(\n",
    "    mat_filename, patch_centres_per_frame\n",
    ")\n",
    "foa_centres_per_frame = [frame[target_subject] for frame in foa_centres_per_frame_per_subject]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_markov_env = FlatFramesEnvironment(\n",
    "    1,\n",
    "    video_frames,\n",
    "    dynamics_frames,\n",
    "    patches_frames,\n",
    "    patch_bounding_boxes_per_frame,\n",
    "    patch_centres_per_frame,\n",
    "    speaker_info_per_frame,\n",
    "    foa_centres_per_frame,\n",
    "    patch_weights_per_frame,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_envs = 5\n",
    "num_test_envs = 3\n",
    "\n",
    "train_envs = ts.env.DummyVectorEnv([lambda: flat_markov_env for _ in range(num_train_envs)])\n",
    "test_envs = ts.env.DummyVectorEnv([lambda: flat_markov_env for _ in range(num_test_envs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNet(nn.Module):\n",
    "    def __init__(self, observation_shape, action_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        # Tianshou needs this attribute for GAIL to work\n",
    "        self.output_dim = np.prod(action_shape)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(observation_shape, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, np.prod(action_shape), bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, state=None, info={}):\n",
    "        if not isinstance(obs, torch.Tensor):\n",
    "            obs = torch.tensor(obs, dtype=torch.float32)\n",
    "        \n",
    "        obs = obs.view(obs.size(0), -1)  # Ensure obs is flat\n",
    "\n",
    "        logits = self.net(obs)\n",
    "\n",
    "        return logits, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNet(nn.Module):\n",
    "    def __init__(self, observation_shape):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(observation_shape, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 1, bias=False)  # Output is the value function for the critic\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, state=None, info={}):\n",
    "        # Directly use the flat observation array\n",
    "        # Ensure obs is a torch.Tensor\n",
    "        if not isinstance(obs, torch.Tensor):\n",
    "            obs = torch.tensor(obs, dtype=torch.float32)\n",
    "        \n",
    "        obs = obs.view(obs.size(0), -1)\n",
    "\n",
    "        state_value = self.net(obs)\n",
    "\n",
    "        return state_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need a further network for GAIL: the discriminator network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorNet(nn.Module):\n",
    "    def __init__(self, observation_shape, action_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        action_size = np.prod(action_shape)\n",
    "\n",
    "        self.device = torch.device(\"cpu\")\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(observation_shape + action_size, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 1, bias=False)  # the probability of being from expert\n",
    "        )\n",
    "\n",
    "    def forward(self, obs_action, state=None, info={}):\n",
    "        if not isinstance(obs_action, torch.Tensor):\n",
    "            obs_action = torch.tensor(obs_action, dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        obs_action = obs_action.view(obs_action.size(0), -1)\n",
    "\n",
    "        prob = self.net(obs_action)\n",
    "\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_markov_env.reset() # this doesn't distrub anything: Tianshou resets the environment before each run\n",
    "original_shapes = flat_markov_env._get_info()[\"shapes\"]\n",
    "state_shape = sum(np.prod(shape) for shape in original_shapes.values())\n",
    "\n",
    "action_shape = 1 # a Discrete value\n",
    "\n",
    "actor_net = ActorNet(state_shape, action_shape)\n",
    "critic_net = CriticNet(state_shape)\n",
    "discriminator_net = DiscriminatorNet(state_shape, action_shape)\n",
    "\n",
    "# using a single optimizer for actor and critic simplifies the training loop and is more computationally efficient (plus it guarantees some consistency across networks)\n",
    "# BUT gradient updates in one network will influence the gradient updates in the other, and this might create unexpected problems...\n",
    "combined_params = list(actor_net.parameters()) + list(critic_net.parameters())\n",
    "optimizer = torch.optim.Adam(combined_params, lr=3e-4)\n",
    "\n",
    "disc_params = list(discriminator_net.parameters())\n",
    "disc_optimizer = torch.optim.Adam(disc_params, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorNet(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=403214, out_features=128, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=64, out_features=1, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CriticNet(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=403214, out_features=128, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=64, out_features=1, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critic_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiscriminatorNet(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=403215, out_features=128, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=64, out_features=1, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up GAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expert_policy(info):\n",
    "    next_frame_idx = info[\"current_frame_idx\"] + 1\n",
    "    attended_centre = foa_centres_per_frame[next_frame_idx]\n",
    "\n",
    "    # always picks the correct patch\n",
    "    return np.array(patch_centres_per_frame[next_frame_idx].index(attended_centre))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need a buffer of expert data, which is meant to be imitated by our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 1000 \n",
    "expert_buffer = ts.data.ReplayBuffer(size=buffer_size)\n",
    "\n",
    "obs, info = flat_markov_env.reset()\n",
    "for _ in range(buffer_size):\n",
    "    action = expert_policy(info) \n",
    "\n",
    "    next_obs, _, terminated, truncated, next_info = flat_markov_env.step(action)\n",
    "    \n",
    "    # the reward is not used by GAIL, but is necessary, as per Tianshou API\n",
    "    expert_buffer.add(ts.data.Batch(obs=obs, act=action, rew=0.0, obs_next=next_obs, terminated=terminated, truncated=truncated))\n",
    "    \n",
    "    if terminated:\n",
    "        # reset the environment if we terminate before filling the buffer\n",
    "        obs, info = flat_markov_env.reset()\n",
    "    else:\n",
    "        obs, info = next_obs, next_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'DiscriminatorNet' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdist_fn\u001b[39m(logits: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mCategorical(logits\u001b[38;5;241m=\u001b[39mlogits)\n\u001b[0;32m----> 4\u001b[0m policy \u001b[38;5;241m=\u001b[39m \u001b[43mGAILPrimePolicy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactor_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcritic_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdist_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpert_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiscriminator_net\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisc_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/uni/affectivecompute/project/lib/python3.11/site-packages/tianshou/policy/modelfree/ppo.py:78\u001b[0m, in \u001b[0;36mPPOPolicy.__init__\u001b[0;34m(self, actor, critic, optim, dist_fn, eps_clip, dual_clip, value_clip, advantage_normalization, recompute_advantage, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(actor, critic, optim, dist_fn, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eps_clip \u001b[38;5;241m=\u001b[39m eps_clip\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m dual_clip \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mdual_clip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m, \\\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDual-clip PPO parameter should greater than 1.0.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dual_clip \u001b[38;5;241m=\u001b[39m dual_clip\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value_clip \u001b[38;5;241m=\u001b[39m value_clip\n",
      "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'DiscriminatorNet' and 'float'"
     ]
    }
   ],
   "source": [
    "def dist_fn(logits: torch.Tensor):\n",
    "    return torch.distributions.Categorical(logits=logits)\n",
    "\n",
    "policy = GAILPrimePolicy(\n",
    "    actor_net, \n",
    "    critic_net, \n",
    "    optimizer,\n",
    "    dist_fn,\n",
    "    expert_buffer,\n",
    "    discriminator_net,\n",
    "    disc_optimizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_collector = ts.data.Collector(policy, train_envs)\n",
    "\n",
    "test_collector = ts.data.Collector(policy, test_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "num_steps_per_epoch = 500\n",
    "step_per_collect = 5\n",
    "episode_per_test = 3\n",
    "batch_size = 10\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%d%m%Y-%H%M%S\")\n",
    "log_path = os.path.join(\"logs\", \"gail\", \"frames\", f\"video_{vid_filename}\", f\"subject_{target_subject}\", timestamp)\n",
    "writer = SummaryWriter(log_path)\n",
    "logger = TensorboardLogger(writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1:  17%|#7        | 85/500 [00:18<01:28,  4.68it/s, env_step=85, len=0, loss=9.934, loss/clip=0.000, loss/disc=4.656, loss/ent=0.000, loss/vf=19.867, n/ep=0, n/st=5, rew=0.00, stats/acc_exp=0.553, stats/acc_pi=0.494]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monpolicy_trainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_collector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_collector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepeat_per_collect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_steps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_per_collect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_per_collect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepisode_per_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepisode_per_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/uni/affectivecompute/project/lib/python3.11/site-packages/tianshou/trainer/onpolicy.py:150\u001b[0m, in \u001b[0;36monpolicy_trainer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21monpolicy_trainer\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]:  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for OnpolicyTrainer run method.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    It is identical to ``OnpolicyTrainer(...).run()``.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m    :return: See :func:`~tianshou.trainer.gather_info`.\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mOnpolicyTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/uni/affectivecompute/project/lib/python3.11/site-packages/tianshou/trainer/base.py:441\u001b[0m, in \u001b[0;36mBaseTrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 441\u001b[0m     \u001b[43mdeque\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# feed the entire iterator into a zero-length deque\u001b[39;00m\n\u001b[1;32m    442\u001b[0m     info \u001b[38;5;241m=\u001b[39m gather_info(\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_time, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_collector, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_collector,\n\u001b[1;32m    444\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_reward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_reward_std\n\u001b[1;32m    445\u001b[0m     )\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/uni/affectivecompute/project/lib/python3.11/site-packages/tianshou/trainer/base.py:288\u001b[0m, in \u001b[0;36mBaseTrainer.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    286\u001b[0m result: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_collector \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 288\u001b[0m     data, result, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_fn_flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     t\u001b[38;5;241m.\u001b[39mupdate(result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn/st\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_fn_flag:\n",
      "File \u001b[0;32m~/Desktop/uni/affectivecompute/project/lib/python3.11/site-packages/tianshou/trainer/base.py:384\u001b[0m, in \u001b[0;36mBaseTrainer.train_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_fn:\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv_step)\n\u001b[0;32m--> 384\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_collector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_per_collect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_episode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepisode_per_collect\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn/ep\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward_metric:\n\u001b[1;32m    388\u001b[0m     rew \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward_metric(result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrews\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/Desktop/uni/affectivecompute/project/lib/python3.11/site-packages/tianshou/data/collector.py:279\u001b[0m, in \u001b[0;36mCollector.collect\u001b[0;34m(self, n_step, n_episode, random, render, no_grad, gym_reset_kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m no_grad:\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# faster than retain_grad version\u001b[39;00m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;66;03m# self.data.obs will be used by agent to get result\u001b[39;00m\n\u001b[0;32m--> 279\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    281\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, last_state)\n",
      "File \u001b[0;32m~/Desktop/uni/affectivecompute/project/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/uni/affectivecompute/project/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/uni/affectivecompute/project/lib/python3.11/site-packages/tianshou/policy/modelfree/pg.py:120\u001b[0m, in \u001b[0;36mPGPolicy.forward\u001b[0;34m(self, batch, state, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    102\u001b[0m     batch: Batch,\n\u001b[1;32m    103\u001b[0m     state: Optional[Union[\u001b[38;5;28mdict\u001b[39m, Batch, np\u001b[38;5;241m.\u001b[39mndarray]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    105\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Batch:\n\u001b[1;32m    106\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute action over the given batch data.\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m    :return: A :class:`~tianshou.data.Batch` which has 4 keys:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m        more detailed explanation.\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m     logits, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(logits, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    122\u001b[0m         dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdist_fn(\u001b[38;5;241m*\u001b[39mlogits)\n",
      "File \u001b[0;32m~/Desktop/uni/affectivecompute/project/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/uni/affectivecompute/project/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 22\u001b[0m, in \u001b[0;36mActorNet.forward\u001b[0;34m(self, obs, state, info)\u001b[0m\n\u001b[1;32m     18\u001b[0m     obs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(obs, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     20\u001b[0m obs \u001b[38;5;241m=\u001b[39m obs\u001b[38;5;241m.\u001b[39mview(obs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Ensure obs is flat\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits, state\n",
      "File \u001b[0;32m~/Desktop/uni/affectivecompute/project/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/uni/affectivecompute/project/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/uni/affectivecompute/project/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/uni/affectivecompute/project/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/uni/affectivecompute/project/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/uni/affectivecompute/project/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result = ts.trainer.onpolicy_trainer(\n",
    "    policy, \n",
    "    train_collector, \n",
    "    test_collector,\n",
    "    repeat_per_collect=1,\n",
    "    max_epoch=num_epochs,\n",
    "    step_per_epoch=num_steps_per_epoch,\n",
    "    step_per_collect=step_per_collect,\n",
    "    episode_per_test=episode_per_test,\n",
    "    batch_size=batch_size,\n",
    "    logger=logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'duration': '190.98s',\n",
       " 'train_time/model': '142.87s',\n",
       " 'test_step': 4356,\n",
       " 'test_episode': 22,\n",
       " 'test_time': '26.18s',\n",
       " 'test_speed': '166.40 step/s',\n",
       " 'best_reward': 89.12511129030099,\n",
       " 'best_result': '89.13 ± 1.63',\n",
       " 'train_step': 5000,\n",
       " 'train_episode': 10,\n",
       " 'train_time/collector': '21.93s',\n",
       " 'train_speed': '30.34 step/s'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n/ep': 10,\n",
       " 'n/st': 3162,\n",
       " 'rews': array([ 34.12632742,  27.83506823,  38.06989051,  45.2986403 ,\n",
       "         42.55142094,  40.5135169 , 254.14732912, 233.32957745,\n",
       "        362.57293248, 292.67269146]),\n",
       " 'lens': array([ 79,  79,  79,  79,  79,  79, 573, 573, 771, 771]),\n",
       " 'idxs': array([4, 4, 4, 4, 4, 4, 2, 3, 0, 1]),\n",
       " 'rew': 137.111739480876,\n",
       " 'len': 316.2,\n",
       " 'rew_std': 125.31813530663396,\n",
       " 'len_std': 297.1803492830574}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.eval()\n",
    "\n",
    "collector = ts.data.Collector(policy, train_envs)\n",
    "collector.collect(n_episode=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
